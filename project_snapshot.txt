File Hierarchy:
README.md
config.ini
requirements.txt
src/
src/__init__.py
src/auth.py
src/config.py
src/google_sheets_uploader.py
src/gui.py
src/gui_theme_definition.txt
src/logger.py
src/main.py
src/models.py
src/scraper.kv
src/utils.py
urls.txt

--- START OF FILE: README.md ---
# Slap Red Scraper Alpha

The Slap Red Scraper Alpha is a Python-based tool designed to automate the collection of bonus and downline data from specified websites. It authenticates with target sites, fetches relevant information, and processes it into a structured format. Key capabilities include logging all operations, storing daily data, maintaining a historical archive of bonus information, generating daily comparison reports to track bonus changes, and providing a rich, dynamic console display for monitoring progress. The scraper is configurable via an INI file for credentials, target URLs, and operational settings.

## Features

*   **Automated Data Scraping**: Fetches bonus and (optionally) downline data from a list of specified URLs.
*   **User Authentication**: Securely logs into target sites using credentials provided in `config.ini`.
*   **Comprehensive Logging**:
    *   Detailed JSON-formatted logs for all significant events, including API requests/responses, errors, and job summaries.
    *   Logs are stored in the `logs/` directory (e.g., `logs/scrape.log`).
*   **Organized Data Output**: All generated data files are stored in the `data/` directory.
    *   **Daily Bonus CSVs**: Raw bonus data for the current day is saved in `data/[mm-dd] bonuses.csv`.
    *   **Historical Bonus Tracking**: Daily bonus CSVs are archived into `data/historical_bonuses.xlsx`, with each day's data on a separate sheet named `mm-dd`.
    *   **Daily Comparison Reports**: A CSV report (`data/comparison_report_[mm-dd].csv`) is generated, comparing the current day's bonuses against the previous day's data from the historical archive. This report categorizes bonuses as "New", "Used", "Persistent_Changed", or "Persistent_Unchanged", including details of what changed for persistent bonuses.
*   **Dynamic Console Display**:
    *   A rich, multi-line progress display updates in real-time in the console.
    *   Includes a graphical progress bar, percentage completion, per-site processing time, and total script run count.
    *   Shows detected bonus type flags (`[C]ommissions, [D]ownline First Deposit, [S]hare, [O]ther`) for each site.
    *   Provides detailed per-site statistics comparing current run vs. previous run for new and total items (Bonuses, Downlines, Errors).
*   **Run Metrics Caching**:
    *   Utilizes `data/run_metrics_cache.json` to store statistics from previous runs (total run count, per-site new/total items). This enhances the contextual information provided in the console display.
*   **Configurable Operation**:
    *   Control script behavior via `config.ini`, including credentials, target URL file, downline data fetching (enable/disable), and logging verbosity.
*   **Robust Error Handling**: Includes error detection for network issues, API errors, and data processing problems, with relevant information logged.
*   **Google Sheets Integration (Optional)**: Automatically upload generated reports (daily bonus CSV, the latest sheet from historical bonuses Excel, and the daily comparison report CSV) to a specified Google Spreadsheet.

## Directory Structure

The scraper will automatically create the following directories in the project root if they don't already exist:

*   **`/data/`**: This directory is used to store all data files generated by the scraper.
    *   `[mm-dd] bonuses.csv`: Contains raw bonus data scraped on a specific date.
    *   `historical_bonuses.xlsx`: An Excel workbook where each sheet (named `mm-dd`) is an archive of a day's bonus data.
    *   `comparison_report_[mm-dd].csv`: A daily report comparing the day's bonuses to the previous day's, detailing new, used, and changed bonuses.
    *   `run_metrics_cache.json`: An internal file used by the script to store metrics from previous runs, enabling richer contextual information in the console display.

*   **`/logs/`**: This directory contains the log files generated by the scraper.
    *   `scrape.log` (or as configured in `config.ini`): The primary log file containing detailed JSON-formatted logs of the scraper's operations.

## Prerequisites

*   **Python**: Python 3.8 or higher is recommended. The script utilizes features common in modern Python versions.

## Setup & Configuration

This project is structured as a Python package (the `src` directory). For correct operation, especially when running the script as a module (`python -m src.main`), ensure that an empty file named `src/__init__.py` exists. This file being present allows Python to recognize `src` as a package. (This file should already be included in the repository).

1.  **Clone the Repository (if applicable):**
    If you have obtained the code as a Git repository, clone it to your local machine:
    ```bash
    git clone <repository_url>
    cd slap-red-scraper-alpha # Or your project directory name
    ```
    If you have the files directly, navigate to the project's root directory.

2.  **Install Dependencies:**
    The project uses several Python packages listed in `requirements.txt`. Install them using pip:
    ```bash
    pip install -r requirements.txt
    ```
    This will typically install packages like `requests`, `pandas`, and `openpyxl`.

3.  **Configure `config.ini`:**
    The main configuration for the scraper is done through the `config.ini` file located in the root directory of the project. Below is a description of each section and its parameters:

    *   **`[credentials]`**:
        *   `mobile`: Your mobile number used for logging into the target sites.
        *   `password`: Your password for the target sites.

    *   **`[settings]`**:
        *   `file`: The name of the text file containing the list of URLs to scrape (one URL per line). Example: `urls.txt`. This file should be in the root directory.
        *   `downline`: Set to `True` to fetch downline data, or `False` to fetch bonus data.

    *   **`[logging]`**:
        *   `log_file`: Path to the log file. It's recommended to use the default `logs/scrape.log` to store logs in the `logs` directory.
        *   `log_level`: The minimum logging level to record. Options include `DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`.
        *   `console`: Set to `True` to enable logging output to the console in addition to the log file. Set to `False` to disable console logging (note: the dynamic progress display will still print to console).
        *   `detail`: Controls the verbosity of structured log events. Options:
            *   `LESS`: Logs essential events like job start/complete and critical errors.
            *   `MORE`: Includes events like API requests/responses, successful fetches, and CSV writes.
            *   `MAX`: (Currently similar to `MORE`) Potentially for even more detailed future logging.

    *   **`[google_sheets]` (Optional)**:
        *   `enabled`: Set to `True` to enable uploading data to Google Sheets. Set to `False` to disable. Defaults to `False` if not specified.
        *   `credentials_file`: Path to your Google Cloud service account JSON key file. This file is necessary for authentication with the Google Sheets API. Example: `path/to/your/google_credentials.json`.
        *   `spreadsheet_id`: The ID of the Google Spreadsheet where data will be uploaded. You can find this ID in the URL of your spreadsheet (e.g., `https://docs.google.com/spreadsheets/d/SPREADSHEET_ID/edit`).
        *   `upload_daily_bonus`: Set to `True` to upload the daily bonus report (e.g., `data/[mm-dd] bonuses.csv`) as a new sheet. Defaults to `True` if `enabled` is `True`. The sheet will be named `DailyBonus_YYYY-MM-DD`.
        *   `upload_historical_bonus`: Set to `True` to upload the most recent day's data from `data/historical_bonuses.xlsx` (which is a copy of that day's daily bonus CSV) as a new sheet. Defaults to `True` if `enabled` is `True`. The sheet will be named `MM-DD` (matching the Excel sheet name).
        *   `upload_comparison_report`: Set to `True` to upload the daily comparison report (e.g., `data/comparison_report_[mm-dd].csv`) as a new sheet. Defaults to `True` if `enabled` is `True`. The sheet will be named `ComparisonReport_YYYY-MM-DD`.

    Ensure `config.ini` is correctly filled out before running the scraper.

## Setting up Google Sheets Integration

If you want to use the Google Sheets integration feature, you'll need to set up Google API credentials:

1.  **Create a Google Cloud Project:**
    *   Go to the [Google Cloud Console](https://console.cloud.google.com/).
    *   Create a new project or select an existing one.

2.  **Enable APIs:**
    *   In your Google Cloud Project, navigate to "APIs & Services" > "Library".
    *   Search for and enable the **Google Sheets API**.
    *   Search for and enable the **Google Drive API** (this is often required for managing spreadsheets, including creating new ones or accessing existing ones, depending on the operations performed by the `gspread` library).

3.  **Create a Service Account:**
    *   Navigate to "APIs & Services" > "Credentials".
    *   Click "Create Credentials" and select "Service account".
    *   Fill in the service account details (name, ID, description).
    *   Grant appropriate roles. For writing to Google Sheets, "Editor" access to the specific sheet is managed via sharing, but you might consider roles like "Service Account User" or roles related to Drive/Sheets if broader API access is needed for the service account itself (though typically not required for just `gspread` operations on a shared sheet).
    *   Click "Done".

4.  **Download JSON Key:**
    *   After creating the service account, find it in the "Credentials" list.
    *   Click on the service account email.
    *   Go to the "Keys" tab.
    *   Click "Add Key" > "Create new key".
    *   Select "JSON" as the key type and click "Create".
    *   A JSON file will be downloaded. This is your `credentials_file`. Store it securely and update its path in `config.ini`.

5.  **Share the Google Spreadsheet:**
    *   Open the Google Spreadsheet you want the script to upload data to.
    *   Click the "Share" button (usually in the top right corner).
    *   In the "Add people and groups" field, enter the email address of the service account you created (e.g., `your-service-account-name@your-project-id.iam.gserviceaccount.com`).
    *   Grant it "Editor" permissions.
    *   Click "Send" or "Share".

Once these steps are completed and `config.ini` is updated with the `spreadsheet_id` and the correct path to the `credentials_file`, the scraper will be able to upload data to your Google Sheet if `enabled = True` in the `[google_sheets]` section.

## Running the Scraper

After setting up your Python environment, installing dependencies, and configuring `config.ini`, it's important to run the scraper in a way that Python correctly recognizes its package structure (due to internal relative imports like `from .models import ...`).

**Recommended Method (from the project's root directory, e.g., `C:\py\`):**
```bash
python -m src.main
```
This command tells Python to run the `main.py` script as part of the `src` package, which ensures that relative imports within the `src` package work correctly.

**Alternative Method (may cause `ImportError`):**
You might also try running the script directly:
```bash
python src/main.py
```
However, this method can lead to an `ImportError: attempted relative import with no known parent package` if Python doesn't recognize `src` as a package from `main.py`'s perspective. This often happens if the `src` directory (or its parent) is not automatically added to Python's path in a way that resolves the package context for direct script execution.

**Upon execution (using the recommended method), the scraper will:**
1.  Read its configuration from `config.ini`.
2.  (The script relies on `src` being a package, typically ensured by an `src/__init__.py` file, for imports to function correctly.)
3.  Attempt to log in to each URL listed in your specified URL file.
4.  Fetch data (bonuses or downlines, based on `config.ini`).
5.  Display progress dynamically in the console.
6.  Save output data to the `/data` directory.
7.  Log operations to the `/logs` directory.

## Understanding the Output

The scraper produces output in two main forms: the dynamic console display during execution, and various files saved to the `/logs` and `/data` directories.

### Console Display

During execution, the scraper provides a 3-line dynamic display that updates for each URL being processed:

*   **Line 1: Overall Progress**
    *   `| █████----- | [ 50.00%] 10/20 |`
    *   Shows a text-based progress bar (default 40 characters wide, updating with block characters).
    *   Displays the percentage completion and the count of processed URLs versus the total.

*   **Line 2: Current Site & Run Info**
    *   `| 1.6s | [Run #5] | [C] N [D] Y [S] N [O] Y | [URL] https://examplesite.com |`
    *   **Site Processing Time**: Time taken to process the current URL (e.g., `1.6s`).
    *   **Run Count**: Total number of times the script has been executed (e.g., `[Run #5]`).
    *   **Bonus Type Flags**: Indicates types of bonuses found on the current site:
        *   `[C]`: Commissions (Y/N)
        *   `[D]`: Downline First Deposit bonuses (Y/N)
        *   `[S]`: Share bonuses (Y/N)
        *   `[O]`: Other types of bonuses (Y/N)
    *   **URL**: The `cleaned_url` currently being processed.

*   **Line 3: Per-Site Statistics**
    *   `| [B]|[R]:10/5(+5) [T]:100/90(+10) | [D]|[R]:-/- [T]:-/- | [E]|[R]:0/1(-1) [T]:5/6(-1) |`
    *   This line shows statistics for **B**onuses, **D**ownlines, and **E**rrors for the current site. Downline stats are only shown if `downline_enabled = True` in `config.ini`.
    *   **`[R]: cr/pr(±diff)`** (Run figures for this site):
        *   `cr`: New items found for this site in the current script run.
        *   `pr`: New items found for this site in the previous script run.
        *   `±diff`: Change between current and previous run's new items.
    *   **`[T]: crt/prt(±diff)`** (Total figures for this site):
        *   `crt`: Cumulative total items for this site up to and including the current run.
        *   `prt`: Cumulative total items for this site up to and including the previous run.
        *   `±diff`: Change in cumulative totals (should equal `cr`).
    *   A `-` (e.g., `[R]:-`) indicates that both current and previous values for that specific part (R or T) were zero, meaning no activity to report for that sub-metric.

### Log Files

*   **Location**: `logs/` directory (e.g., `logs/scrape.log`).
*   **Format**: JSON lines. Each line is a JSON object representing a log event.
*   **Content**: Detailed information about script operations, including API calls, errors, data fetching summaries, and job start/completion times. Useful for debugging and tracking.

### Data Files

All data files are stored in the `data/` directory.

*   **`[mm-dd] bonuses.csv`**:
    *   A CSV file created daily, containing all bonuses scraped on that particular date (`mm-dd`).
    *   Columns correspond to the fields of the `Bonus` data model (e.g., `url`, `merchant_name`, `id`, `name`, `amount`, `rollover`, etc.).

*   **`historical_bonuses.xlsx`**:
    *   An Excel workbook that serves as an archive of all daily bonus data.
    *   Each sheet in the workbook is named with the date (`mm-dd`) and contains the bonus data from that day (copied from the daily CSV).
    *   This allows for easy historical review and analysis.

*   **`comparison_report_[mm-dd].csv`**:
    *   A CSV file generated daily, providing a comparison of the current day's bonuses against the previous day's data (from `historical_bonuses.xlsx`).
    *   Key columns include:
        *   `status`: Indicates if a bonus is "New", "Used" (present yesterday, gone today), "Persistent_Changed", or "Persistent_Unchanged".
        *   `change_details`: For "Persistent_Changed" bonuses, this column lists the fields that changed and their old vs. new values (e.g., "amount: 10.0 -> 12.0; rollover: 1.0 -> 1.5").
        *   All original bonus data fields are also included.

*   **`run_metrics_cache.json`**:
    *   An internal file used by the script to maintain state between executions.
    *   Stores the `total_script_runs` count and, for each site, the new and total item counts from its previous run. This data is essential for the contextual statistics shown in the console display. It's not typically meant for direct user consumption but is vital for the script's enhanced display features.
--- END OF FILE: README.md ---
--- START OF FILE: config.ini ---
[credentials]
mobile = 61423349819
password = Falcon66!

[settings]
file = urls.txt
downline = False

[logging]
log_file = logs/scrape.log
log_level = DEBUG
console = True
detail = MORE

[google_sheets]
enabled = false
credentials_file = path/to/your/google_credentials.json
spreadsheet_id = your_spreadsheet_id
upload_daily_bonus = true
upload_historical_bonus = true
upload_comparison_report = true
--- END OF FILE: config.ini ---
--- START OF FILE: requirements.txt ---
gspread
google-auth
--- END OF FILE: requirements.txt ---
--- START OF FILE: src/__init__.py ---

--- END OF FILE: src/__init__.py ---
--- START OF FILE: src/auth.py ---
import re
import requests
from typing import Optional
from .models import AuthData
from .logger import Logger

class AuthService:
    """Manages authentication and URL processing."""
    API_PATH = "/api/v1/index.php"

    def __init__(self, logger: Logger):
        self.logger = logger

    @staticmethod
    def clean_url(url: str) -> str:
        return re.sub(r"/\w+$", "", url)

    @staticmethod
    def extract_merchant_info(html: str) -> tuple[Optional[str], Optional[str]]:
        match = re.search(r'var MERCHANTID = (\d+);\s*var MERCHANTNAME = "(.*?)";', html)
        return match.groups() if match else (None, None)

    def login(self, url: str, mobile: str, password: str) -> Optional[AuthData]:
        try:
            response = requests.get(url)
            response.raise_for_status()
            html = response.text
        except Exception as e:
            self.logger.emit("exception", {"error": f"Failed to fetch URL {url}: {str(e)}"})
            return None

        merchant_id, merchant_name = self.extract_merchant_info(html)
        if not merchant_id:
            self.logger.emit("exception", {"error": f"No merchant ID found for {url}"})
            return None

        api_url = url + self.API_PATH
        payload = {
            "module": "/users/login",
            "mobile": mobile,
            "password": password,
            "merchantId": merchant_id,
            "domainId": "0",
            "accessId": "",
            "accessToken": "",
            "walletIsAdmin": ""
        }

        # Log the API request with non-sensitive parts of the payload
        self.logger.emit("api_request", {
            "url": api_url,
            "action": "login", # Specific to this login action
            "module": payload.get("module"), # Consistent with other api_request logs
            "mobile": payload.get("mobile") # Non-sensitive identifier
        })

        try:
            response = requests.post(api_url, data=payload)
            response.raise_for_status()
            # Assuming the response is JSON. If not, this will raise an error caught by the except block.
            res_json = response.json()

            # Log the API response
            response_details = {"url": api_url, "action": "login", "status": res_json.get("status")}
            if res_json.get("status") != "SUCCESS":
                if res_json.get("message"):
                    response_details["error_message"] = res_json.get("message")
                if isinstance(res_json.get("data"), dict) and res_json.get("data", {}).get("description"):
                     response_details["error_description"] = res_json.get("data").get("description")
                elif isinstance(res_json.get("data"), str):
                     response_details["error_data_string"] = res_json.get("data")
            self.logger.emit("api_response", response_details)

            data = res_json.get("data", {})
            if not data.get("token"): # Check based on expected success criteria
                self.logger.emit("login_failed", {"url": url, "reason": res_json.get("message", "No token in response")})
                return None

            self.logger.emit("login_success", {"url": url})
            return AuthData(
                merchant_id=merchant_id,
                merchant_name=merchant_name,
                access_id=data.get("id"),
                token=data.get("token"),
                api_url=api_url
            )
        except Exception as e:
            self.logger.emit("exception", {"error": f"Login failed for {url}: {str(e)}"})
            return None
--- END OF FILE: src/auth.py ---
--- START OF FILE: src/config.py ---
from dataclasses import dataclass
from typing import Optional
import configparser
import os
import sys

@dataclass
class Credentials:
    mobile: str
    password: str

@dataclass
class Settings:
    url_file: str
    downline_enabled: bool

@dataclass
class LoggingConfig:
    log_file: str
    log_level: str
    console: bool
    detail: str

@dataclass
class GoogleSheetsConfig:
    enabled: bool
    credentials_file: str
    spreadsheet_id: str
    upload_daily_bonus: bool
    upload_historical_bonus: bool
    upload_comparison_report: bool

@dataclass
class AppConfig:
    credentials: Credentials
    settings: Settings
    logging: LoggingConfig
    google_sheets: GoogleSheetsConfig

class ConfigLoader:
    """Loads and validates configuration from a .ini file."""
    def __init__(self, path: str = "config.ini"):
        if not os.path.exists(path):
            sys.exit(f"Configuration file not found: {path}")
        self.config = configparser.ConfigParser()
        self.config.read(path)

    def load(self) -> AppConfig:
        try:
            gs_config = self.config["google_sheets"]
            return AppConfig(
                credentials=Credentials(
                    mobile=self.config["credentials"]["mobile"],
                    password=self.config["credentials"]["password"]
                ),
                settings=Settings(
                    url_file=self.config["settings"]["file"],
                    downline_enabled=self.config["settings"].getboolean("downline", fallback=False)
                ),
                logging=LoggingConfig(
                    log_file=self.config["logging"]["log_file"],
                    log_level=self.config["logging"]["log_level"],
                    console=self.config["logging"].getboolean("console", fallback=True),
                    detail=self.config["logging"].get("detail", "LESS").upper()
                ),
                google_sheets=GoogleSheetsConfig(
                    enabled=gs_config.getboolean("enabled", fallback=False),
                    credentials_file=gs_config["credentials_file"],
                    spreadsheet_id=gs_config["spreadsheet_id"],
                    upload_daily_bonus=gs_config.getboolean("upload_daily_bonus", fallback=True),
                    upload_historical_bonus=gs_config.getboolean("upload_historical_bonus", fallback=True),
                    upload_comparison_report=gs_config.getboolean("upload_comparison_report", fallback=True)
                )
            )
        except KeyError as e:
            sys.exit(f"Configuration error: Missing key {e}")
--- END OF FILE: src/config.py ---
--- START OF FILE: src/google_sheets_uploader.py ---
import gspread
import pandas as pd
from google.oauth2.service_account import Credentials
from .config import GoogleSheetsConfig # Assuming config is in the same directory or adjust path
from .logger import Logger # Assuming logger is available

class GoogleSheetsUploader:
    def __init__(self, gs_config: GoogleSheetsConfig, logger: Logger):
        self.gs_config = gs_config
        self.logger = logger
        self.client = None
        if self.gs_config.enabled:
            try:
                scopes = [
                    "https.www.googleapis.com/auth/spreadsheets",
                    "https.www.googleapis.com/auth/drive.file" # Required to create new spreadsheets if needed, or just to see files
                ]
                creds = Credentials.from_service_account_file(
                    self.gs_config.credentials_file,
                    scopes=scopes
                )
                self.client = gspread.authorize(creds)
                self.logger.emit("google_sheets_auth_success", {"message": "Successfully authenticated with Google Sheets API."})
            except FileNotFoundError:
                self.logger.emit("google_sheets_auth_error", {"error": f"Credentials file not found: {self.gs_config.credentials_file}"})
                self.client = None # Ensure client is None if auth fails
            except Exception as e:
                self.logger.emit("google_sheets_auth_error", {"error": f"Failed to authenticate with Google Sheets API: {str(e)}"})
                self.client = None # Ensure client is None if auth fails

    def _open_spreadsheet(self):
        if not self.client:
            self.logger.emit("google_sheets_client_error", {"message": "Google Sheets client not initialized."})
            return None
        try:
            spreadsheet = self.client.open_by_id(self.gs_config.spreadsheet_id)
            self.logger.emit("google_sheets_spreadsheet_opened", {"id": self.gs_config.spreadsheet_id})
            return spreadsheet
        except gspread.exceptions.SpreadsheetNotFound:
            self.logger.emit("google_sheets_error", {"error": f"Spreadsheet not found with ID: {self.gs_config.spreadsheet_id}"})
            return None
        except Exception as e:
            self.logger.emit("google_sheets_error", {"error": f"Error opening spreadsheet {self.gs_config.spreadsheet_id}: {str(e)}"})
            return None

    def upload_dataframe(self, df: pd.DataFrame, sheet_name: str):
        if not self.gs_config.enabled or not self.client:
            if not self.gs_config.enabled:
                self.logger.emit("google_sheets_upload_skipped", {"reason": "Google Sheets uploading is disabled in config."})
            return

        spreadsheet = self._open_spreadsheet()
        if not spreadsheet:
            return

        try:
            # Check if worksheet exists
            try:
                worksheet = spreadsheet.worksheet(sheet_name)
                self.logger.emit("google_sheets_worksheet_found", {"name": sheet_name})
            except gspread.exceptions.WorksheetNotFound:
                self.logger.emit("google_sheets_worksheet_creating", {"name": sheet_name})
                worksheet = spreadsheet.add_worksheet(title=sheet_name, rows="1", cols="1") # Create with minimal size

            worksheet.clear() # Clear existing content

            # Convert NaN/NA to empty strings for gspread compatibility if necessary
            df_upload = df.fillna('').astype(str)

            header = [str(col) for col in df_upload.columns.tolist()]
            data_to_upload = [header] + df_upload.values.tolist()

            worksheet.update(data_to_upload, raw=False) # raw=False to parse values
            self.logger.emit("google_sheets_upload_success", {"sheet_name": sheet_name, "rows": len(df_upload)})
        except Exception as e:
            self.logger.emit("google_sheets_upload_error", {"sheet_name": sheet_name, "error": str(e)})

    def upload_csv_file(self, csv_file_path: str, sheet_name: str):
        if not self.gs_config.enabled or not self.client:
            if not self.gs_config.enabled:
                self.logger.emit("google_sheets_upload_skipped", {"reason": "Google Sheets uploading is disabled in config."})
            return

        try:
            df = pd.read_csv(csv_file_path)
            self.upload_dataframe(df, sheet_name)
        except FileNotFoundError:
            self.logger.emit("google_sheets_upload_error", {"sheet_name": sheet_name, "error": f"CSV file not found: {csv_file_path}"})
        except Exception as e:
            self.logger.emit("google_sheets_upload_error", {"sheet_name": sheet_name, "error": f"Error processing CSV {csv_file_path} for upload: {str(e)}"})
--- END OF FILE: src/google_sheets_uploader.py ---
--- START OF FILE: src/gui.py ---
import kivy
from kivy.app import App
from kivy.uix.boxlayout import BoxLayout
from kivy.uix.label import Label
from kivy.uix.button import Button
from kivy.uix.textinput import TextInput
from kivy.uix.screenmanager import ScreenManager, Screen
from kivy.core.window import Window
from kivy.uix.gridlayout import GridLayout
from kivy.uix.checkbox import CheckBox
from kivy.uix.scrollview import ScrollView
from kivy.clock import Clock
from src.logger import Logger
from src.config import ConfigLoader
from kivy.properties import BooleanProperty # For NavButton is_active state
# from src.main import execute_scraping_logic as actual_run_scraper_main # Already imported later
import threading
import configparser
import os

# Define NavButton Python side for the custom property, Kivy will link it to KV rule <NavButton@Button>
class NavButton(Button):
    is_active = BooleanProperty(False)

class ConfigScreen(Screen):
    def __init__(self, **kwargs):
        super(ConfigScreen, self).__init__(**kwargs)
        self.inputs = {}
        # The layout and widgets for ConfigScreen are more dynamic and will remain mostly in Python.
        # KV styles for Label, TextInput, Button, CheckBox will apply.
        self._create_layout()

    def _create_layout(self):
        # This layout creation remains in Python due to its dynamic nature based on config fields
        # However, padding and spacing will come from KV's <GridLayout> rule if not overridden here.
        # Or, assign a class/id and style it in KV: self.layout.style_class = 'ConfigGridLayout'
        self.layout_grid = GridLayout(cols=2) # padding/spacing from KV

        # Load initial configuration
        self.config_loader = ConfigLoader(path="config.ini")
        try:
            if not os.path.exists("config.ini"):
                self.app_config = None
                # Use a label for warnings within the UI if possible, or rely on console print
                print("WARNING: config.ini not found. GUI will show empty fields or defaults.")
                # self.layout_grid.add_widget(Label(text="config.ini not found. Using defaults."))
            else:
                self.app_config = self.config_loader.load()
        except Exception as e: # Catch potential errors from ConfigLoader.load()
            self.app_config = None
            print(f"ERROR: Failed to load config.ini: {e}")
            # self.layout_grid.add_widget(Label(text=f"Error loading config: {e}"))


        # Define fields to be created
        fields = [
            ("Mobile:", "mobile", "credentials.mobile", False, False),
            ("Password:", "password", "credentials.password", True, False),
            ("URL File:", "url_file", "settings.url_file", False, False),
            ("Downline Enabled:", "downline_enabled", "settings.downline_enabled", False, True),
            ("Log File:", "log_file", "logging.log_file", False, False),
            ("Log Level:", "log_level", "logging.log_level", False, False),
            ("Console Logging:", "console", "logging.console", False, True),
            ("Log Detail:", "detail", "logging.detail", False, False),
        ]

        for label_text, key, config_path, is_password, is_checkbox in fields:
            self.layout_grid.add_widget(Label(text=label_text))

            # Resolve path to get value from app_config
            current_value = ""
            if self.app_config:
                try:
                    value_keys = config_path.split('.')
                    temp_val = self.app_config
                    for k in value_keys:
                        if isinstance(temp_val, dict): # If it's a dict from a sub-config
                             temp_val = temp_val.get(k)
                        else: # If it's a dataclass
                            temp_val = getattr(temp_val, k)
                    current_value = temp_val
                except (AttributeError, KeyError):
                    current_value = False if is_checkbox else "" # Default for missing keys

            if is_checkbox:
                widget = CheckBox(active=bool(current_value))
            else:
                widget = TextInput(
                    text=str(current_value),
                    password=is_password,
                    write_tab=False
                )
            self.inputs[key] = widget
            self.layout_grid.add_widget(widget)

        # Save Button - height from KV if Button rule has it, or set here if specific
        self.save_button = Button(text='Save Configuration') # size_hint_y, height from KV
        self.save_button.bind(on_press=self.save_config)
        self.layout_grid.add_widget(Label()) # Placeholder
        self.layout_grid.add_widget(self.save_button)

        # Status Label - height from KV if Label rule has it, or set here
        self.config_status_label = Label(text='') # height from KV
        self.layout_grid.add_widget(self.config_status_label)
        # Add an empty widget to fill the grid if status label doesn't span
        self.layout_grid.add_widget(Label())


        self.add_widget(self.layout_grid)

    def save_config(self, instance):
        # Access status_label via self.config_status_label now
        config_parser = configparser.ConfigParser()

        config_parser['credentials'] = {
            'mobile': self.inputs['mobile'].text,
            'password': self.inputs['password'].text
        }
        config_parser['settings'] = {
            'file': self.inputs['url_file'].text,
            'downline': str(self.inputs['downline_enabled'].active)
        }
        config_parser['logging'] = {
            'log_file': self.inputs['log_file'].text,
            'log_level': self.inputs['log_level'].text,
            'console': str(self.inputs['console'].active),
            'detail': self.inputs['detail'].text
        }

        try:
            with open('config.ini', 'w') as configfile:
                config_parser.write(configfile)
            self.config_status_label.text = "Configuration Saved!"
            # Optionally, clear the message after a few seconds
            Clock.schedule_once(lambda dt: setattr(self.config_status_label, 'text', ""), 5)
        except Exception as e:
            self.config_status_label.text = f"Error saving: {e}"
            print(f"Error saving configuration: {e}")


class ProgressScreen(Screen):
    # KV file now defines the layout. Python class is for logic.
    # __init__ can be minimal or used for non-widget related setup.
    # on_kv_post is useful if you need to access self.ids right after KV parsing.

    # Example: If start_button's on_press needs to be bound in Python
    # def on_kv_post(self, base_widget):
    #    self.ids.start_button.bind(on_press=lambda x: App.get_running_app().start_scraping_thread(self.ids.start_button))
    # However, it's better to define this in the KV file if possible:
    # <ProgressScreen>:
    #     BoxLayout:
    #         Button:
    #             id: start_button
    #             on_press: app.start_scraping_thread(self) # 'self' here is the button

    def add_log_message(self, message):
        # Ensure messages are handled on the main thread
        # Check if self.ids exists, which means KV rules have been applied.
        if hasattr(self, 'ids') and 'log_display' in self.ids:
            log_display_widget = self.ids.log_display
            def append_message(msg):
                log_display_widget.text += msg + "\n"
                if hasattr(self.ids, 'log_display_scroll'):
                    Clock.schedule_once(lambda dt: setattr(self.ids.log_display_scroll, 'scroll_y', 0), 0)

            if threading.current_thread() != threading.main_thread():
                Clock.schedule_once(lambda dt: append_message(message))
            else:
                append_message(message)
        else:
            # Fallback or error if called before ids are populated (should not happen if KV is correct)
            print(f"WARN: ProgressScreen.add_log_message called before ids populated or log_display not in ids. Message: {message}")


    def clear_logs(self):
        if hasattr(self, 'ids') and 'log_display' in self.ids and 'status_label' in self.ids:
            self.ids.log_display.text = ""
            self.ids.status_label.text = ""
        else:
            print("WARN: ProgressScreen.clear_logs called before ids populated.")


    def set_status(self, message):
        if hasattr(self, 'ids') and 'status_label' in self.ids:
            status_label_widget = self.ids.status_label
            def update_status(msg):
                status_label_widget.text = msg

            if threading.current_thread() != threading.main_thread():
                Clock.schedule_once(lambda dt: update_status(message))
            else:
                update_status(message)
        else:
            print(f"WARN: ProgressScreen.set_status called before ids populated. Message: {message}")


class HistoryScreen(Screen):
    # KV file defines the layout. Python class is for logic.
    # __init__ can be minimal.
    # on_kv_post can be used for bindings if not done in KV.
    # e.g. self.ids.refresh_button.bind(on_press=self.load_and_display_metrics)
    # Or in KV: <Button>: id: refresh_button; on_press: root.load_and_display_metrics()

    def __init__(self, **kwargs):
        super(HistoryScreen, self).__init__(**kwargs)
        self.metric_labels = {}

    def display_metrics(self, metrics_data):
        if not hasattr(self, 'ids') or 'metrics_layout' not in self.ids:
            print("WARN: HistoryScreen.display_metrics called before ids populated or metrics_layout not in ids.")
            return

        metrics_layout_widget = self.ids.metrics_layout
        metrics_layout_widget.clear_widgets()
        self.metric_labels.clear()

        # Define the order and display names for metrics
        metric_display_order = {
            "runs": "Total Scraper Runs:",
            "total_runtime": "Total Runtime (seconds):",
            "bonuses": "Total Bonuses Fetched:",
            "total_bonus_amount": "Total Bonus Amount:",
            "downlines": "Total Downlines Fetched:",
            "errors": "Total Errors Logged:",
            "successful_bonus_fetches": "Successful Bonus Fetches (Events):",
            "failed_bonus_api_calls": "Failed Bonus API Calls:"
        }

        for key, display_name in metric_display_order.items():
            value = metrics_data.get(key, "N/A")

            name_label = Label(text=display_name, halign='left') # size_hint_x from KV if needed via class rule
            # name_label.bind(texture_size=name_label.setter('size')) # KV can handle this with size_hint

            value_label = Label(text=str(value), halign='right')
            # value_label.bind(texture_size=value_label.setter('size'))

            metrics_layout_widget.add_widget(name_label)
            metrics_layout_widget.add_widget(value_label)
            self.metric_labels[key] = value_label

    def load_and_display_metrics(self, instance=None):
        metrics_layout_widget = self.ids.metrics_layout
        try:
            cfg_loader = ConfigLoader()
            app_cfg = cfg_loader.load()
            log_file_path = app_cfg.logging.log_file
            logger_instance = Logger(log_file=log_file_path, log_level='INFO', console=False, detail='LESS')
            metrics = logger_instance.load_metrics(log_file_path)
            self.display_metrics(metrics)
        except FileNotFoundError:
            metrics_layout_widget.clear_widgets()
            metrics_layout_widget.add_widget(Label(text="Error: config.ini not found.", color=(1,0,0,1)))
            print("Error: config.ini not found. Cannot load metrics.")
        except KeyError as e:
            metrics_layout_widget.clear_widgets()
            metrics_layout_widget.add_widget(Label(text=f"Config Error: Missing key {e}.", color=(1,0,0,1)))
            print(f"Error: Configuration missing key {e}. Cannot load metrics.")
        except Exception as e:
            metrics_layout_widget.clear_widgets()
            metrics_layout_widget.add_widget(Label(text=f"Error: {e}", color=(1,0,0,1)))
            print(f"An unexpected error occurred: {e}")

    def on_enter(self):
        self.load_and_display_metrics()

class ScraperApp(App):
    # Primary and Highlight colors for Window and other Python-side styling if needed.
    # These are also defined in KV for KV-side styling.
    primary_bg_color = (0.133, 0.133, 0.133, 1)
    highlight_color = (1, 0.2, 0.2, 1)

    def build(self):
        Window.clearcolor = self.primary_bg_color
        Window.minimum_width = 800
        Window.minimum_height = 600
        # Title is already set in KV for ScraperApp if we define a root rule for it,
        # or keep it here. For now, it's set in python.
        Window.title = "Scraper Control Panel"


        # The root layout. KV file will define its structure if we use a root rule.
        # For now, Python builds the main structure (Nav + ScreenManager).
        # KV will style the NavButtons and Screens.
        root_layout = BoxLayout(orientation='vertical') # No padding/spacing here, let KV handle it or use a class

        self.screen_manager = ScreenManager()

        self.config_screen = ConfigScreen(name='config')
        self.progress_screen = ProgressScreen(name='progress')
        self.history_screen = HistoryScreen(name='history')

        self.screen_manager.add_widget(self.config_screen)
        self.screen_manager.add_widget(self.progress_screen)
        self.screen_manager.add_widget(self.history_screen)

        # Navigation Bar
        self.nav_bar = BoxLayout(orientation='horizontal', size_hint_y=None, height='50dp') # Height from KV

        self.nav_buttons = {
            'config': NavButton(text='Configuration'),
            'progress': NavButton(text='Progress'),
            'history': NavButton(text='History')
        }

        self.nav_buttons['config'].bind(on_press=lambda x: self.switch_screen('config'))
        self.nav_buttons['progress'].bind(on_press=lambda x: self.switch_screen('progress'))
        self.nav_buttons['history'].bind(on_press=lambda x: self.switch_screen('history'))

        for name, button in self.nav_buttons.items():
            self.nav_bar.add_widget(button)

        root_layout.add_widget(self.nav_bar)
        root_layout.add_widget(self.screen_manager)

        self.switch_screen('config') # Start on config screen
        return root_layout

    def switch_screen(self, screen_name):
        self.screen_manager.current = screen_name
        for name, button in self.nav_buttons.items():
            button.is_active = (name == screen_name)

    def start_scraping_thread(self, button_instance):
        # Ensure button_instance is the one from self.ids if called from KV
        # If called via lambda from Python-created button, it's direct.
        # The ProgressScreen KV has id 'start_button'.
        # So if ProgressScreen itself binds it, it should use self.ids.start_button.
        # If ScraperApp binds it (as it does now via lambda in ProgressScreen KV for example),
        # the button instance is passed correctly.
        # The 'button_instance' argument IS the button pressed.

        if button_instance:
            button_instance.disabled = True
        else: # Fallback if somehow not passed, though KV should pass it
            fallback_button = self.progress_screen.ids.get('start_button')
            if fallback_button:
                fallback_button.disabled = True

        self.progress_screen.clear_logs()
        self.progress_screen.set_status("INFO: Starting scraping process...")
        # Ensure this method is idempotent or handles multiple clicks if necessary
        # For now, assume button disable/enable handles rapid clicks.

        button_instance.disabled = True
        self.progress_screen.clear_logs()
        self.progress_screen.set_status("INFO: Starting scraping process...")

        log_callback = self.progress_screen.add_log_message

        # Import the refactored main function
        from src.main import execute_scraping_logic as actual_run_scraper_main

        def target_for_thread():
            try:
                actual_run_scraper_main(gui_callback=log_callback)
                self.progress_screen.set_status("INFO: Scraping process completed.")
            except Exception as e:
                log_callback(f"ERROR: Scraping thread failed: {e}")
                self.progress_screen.set_status(f"ERROR: Scraping failed: {e}")
                # Potentially log the full traceback to the GUI log as well
                import traceback
                log_callback(f"TRACEBACK: {traceback.format_exc()}")
            finally:
                # Re-enable button on the main Kivy thread
                if button_instance:
                    Clock.schedule_once(lambda dt: setattr(button_instance, 'disabled', False))
                else: # Fallback
                    fallback_button = self.progress_screen.ids.get('start_button')
                    if fallback_button:
                        Clock.schedule_once(lambda dt: setattr(fallback_button, 'disabled', False))

                # Optionally, refresh history screen data
                if self.history_screen:
                     Clock.schedule_once(lambda dt: self.history_screen.load_and_display_metrics())


        thread = threading.Thread(target=target_for_thread)
        thread.daemon = True # Allow main app to exit even if thread is running
        thread.start()

if __name__ == '__main__':
    # Remove the TestApp, use the main ScraperApp
    ScraperApp().run()
--- END OF FILE: src/gui.py ---
--- START OF FILE: src/gui_theme_definition.txt ---
Overall Look & Feel: Modern, crisp, aero, minimalist.

Color Palette:
  Primary Background: Dark gray (#222222)
  Secondary Background/Accent: Slightly lighter dark gray (#333333)
  Primary Text Color: Light gray / Off-white (#E0E0E0)
  Highlight Color: Red (#FF3333 or #E57373 for text)
  Widget Colors: Variations of dark grays, red for active elements/borders.

Font:
  Style: Clean, sans-serif. Attempt to use Roboto; fallback to system default sans-serif.
  Size:
    Base: 14sp (scalable pixels)
    Small Label: 12sp
    Medium Heading: 18sp
    Large Heading: 24sp

Widget Styling (Conceptual):
  Buttons: Flat or with subtle gradients. Dark gray background (#333333), light gray text (#E0E0E0). Red border (#FF3333) or background change on hover/press.
  Text Inputs: Dark gray background (#222222), light gray text (#E0E0E0). Red (#FF3333) border or glow effect on focus.
  Labels: Light gray text (#E0E0E0).
  ScrollView/TextArea: Primary dark gray background (#222222), light gray text (#E0E0E0). Scrollbars should be thin, secondary dark gray (#333333) track and thumb, or styled to match the theme as closely as Kivy allows.
  CheckBox: Box with dark gray background (#333333) and a thin light gray border (#E0E0E0). Checkmark in red (#FF3333).
  Layouts (e.g., BoxLayout, GridLayout): Utilize padding (e.g., 10dp) and spacing (e.g., 5dp) between elements to ensure a clean, uncluttered visual hierarchy.

Window:
  Attempt to use a borderless window if Kivy allows for straightforward implementation and custom title bar creation.
  If custom title bar is too complex, theme the standard window title bar to match the dark theme (dark background, light text).
  This contributes to the 'aero' (transparent/glass-like effects, though true aero might be hard) and minimalist feel.
  Minimum window size: e.g., 800x600 pixels.
--- END OF FILE: src/gui_theme_definition.txt ---
--- START OF FILE: src/logger.py ---
import logging
import json
from typing import Dict, Any
from datetime import datetime
import os # Added import os

class JsonFormatter(logging.Formatter):
    def format(self, record: logging.LogRecord) -> str:
        log_record = {
            "timestamp": self.formatTime(record, "%Y-%m-%d %H:%M:%S"),
            "level": record.levelname,
            "module": record.module,
            "method": record.funcName,
            "event": record.msg,
            "details": record.details_data if hasattr(record, 'details_data') else {}
        }
        return json.dumps(log_record)

class Logger:
    """Structured logger with verbosity control."""
    VERBOSITY_LEVELS = {"LESS": 0, "MORE": 1, "MAX": 2}
    EVENT_VERBOSITY = {
        "job_start": "LESS",
        "job_complete": "LESS",
        "login_success": "MORE",
        "login_failed": "MORE",
        "api_request": "MORE", # Changed from MAX
        "api_response": "MORE", # Changed from MAX
        "bonus_fetched": "MORE",
        "downline_fetched": "MORE",
        "csv_written": "MORE",
        "exception": "LESS",
        "website_unresponsive": "LESS",
        "down_sites_summary": "LESS",
        "bonus_api_error": "MORE",
        "progress_update": "LESS" # Added for progress stats display
    }

    def __init__(self, log_file: str, log_level: str, console: bool, detail: str, gui_callback=None):
        self.logger = logging.getLogger("ScraperLogger")
        self.logger.setLevel(getattr(logging, log_level.upper(), logging.DEBUG)) # ensure log_level is upper
        self.verbosity = self.VERBOSITY_LEVELS.get(detail.upper(), 0) # ensure detail is upper
        self.gui_callback = gui_callback

        # File handler
        os.makedirs(os.path.dirname(log_file), exist_ok=True)
        file_handler = logging.FileHandler(log_file)
        file_handler.setFormatter(JsonFormatter())
        self.logger.addHandler(file_handler)

        # Console handler
        if console:
            console_handler = logging.StreamHandler()
            console_handler.setFormatter(logging.Formatter(
                "%(asctime)s [%(levelname)s] %(message)s"
            ))
            self.logger.addHandler(console_handler)

    def emit(self, event: str, details: Dict[str, Any], level: str = "INFO") -> None:
        required_level = self.VERBOSITY_LEVELS.get(self.EVENT_VERBOSITY.get(event, "MORE"), 0)
        if self.verbosity >= required_level:
            # Ensure details is a dictionary for the formatter
            actual_details = details if isinstance(details, dict) else {"data": details}
            self.logger.log(getattr(logging, level.upper()), event, args=(), extra={'details_data': actual_details})

            if self.gui_callback:
                # For "progress_update", the details dictionary is expected to have a "message" key
                # containing the pre-formatted string.
                if event == "progress_update" and "message" in actual_details:
                    gui_message = actual_details["message"] # Send the raw message for progress updates
                else:
                    # Standard formatting for other events
                    gui_message = f"[{level.upper()}] {event}"
                    if actual_details:
                        try:
                            details_str = json.dumps(actual_details)
                        except TypeError: # In case details are not JSON serializable
                            details_str = str(actual_details)
                        gui_message += f": {details_str}"
                self.gui_callback(gui_message)


    def load_metrics(self, log_file: str) -> Dict[str, float]:
        metrics = {
            "bonuses": 0, # Total count of individual bonus items
            "downlines": 0,
            "errors": 0, # General errors + unresponsive + API errors that lead to "ERROR" return
            "runs": 0,
            "total_runtime": 0.0,
            "total_bonus_amount": 0.0,         # Sum of amounts from all bonus_fetched events
            "successful_bonus_fetches": 0,     # Number of times bonus_fetched event occurred
            "failed_bonus_api_calls": 0        # Number of times bonus_api_error event occurred
        }
        if not os.path.exists(log_file): # Check if log_file exists
            return metrics
        with open(log_file, "r") as f:
            for line in f:
                try:
                    log = json.loads(line)
                    event = log.get("event")
                    details = log.get("details", {}) # Ensure details is always a dict

                    if event == "bonus_fetched":
                        metrics["bonuses"] += details.get("count", 0)
                        metrics["total_bonus_amount"] += details.get("total_amount", 0.0)
                        metrics["successful_bonus_fetches"] += 1
                    elif event == "downline_fetched":
                        metrics["downlines"] += details.get("count", 0)
                    elif event == "exception" or event == "website_unresponsive": # Count these as errors for historical load
                        metrics["errors"] += 1
                    elif event == "bonus_api_error":
                        metrics["failed_bonus_api_calls"] += 1
                        metrics["errors"] += 1 # Also count as a general error for overall error tracking
                    elif event == "job_complete":
                        metrics["runs"] += 1
                        metrics["total_runtime"] += details.get("duration", 0.0)
                except json.JSONDecodeError:
                    continue
        return metrics
--- END OF FILE: src/logger.py ---
--- START OF FILE: src/main.py ---
import csv
import os
import sys # Added sys import
import time # Added time import
import requests
import pandas as pd # Added pandas import
from datetime import datetime, timedelta # Added import
from typing import List, Set, Tuple, Union # Union for return types
from .models import Downline, Bonus, AuthData
from .logger import Logger
from .auth import AuthService # Added import for AuthService
from .utils import progress, load_run_cache, save_run_cache # Added cache imports
from .google_sheets_uploader import GoogleSheetsUploader # Added for Google Sheets

class Scraper:
    """Handles scraping of downlines and bonuses."""
    def __init__(self, logger: Logger, request_timeout: int):
        self.logger = logger
        self.request_timeout = request_timeout

    def fetch_downlines(self, url: str, auth: AuthData, csv_file: str = "downlines.csv") -> Union[int, str]:
        written: Set[Tuple] = set()
        if os.path.exists(csv_file):
            with open(csv_file, newline="", encoding="utf-8") as f:
                reader = csv.DictReader(f)
                written = {tuple(row.values()) for row in reader}

        total_new_rows = 0
        page = 0
        while True:
            payload = {
                "level": "1",
                "pageIndex": str(page),
                "module": "/referrer/getDownline",
                "merchantId": auth.merchant_id,
                "domainId": "0",
                "accessId": auth.access_id,
                "accessToken": auth.token,
                "walletIsAdmin": True
            }
            self.logger.emit("api_request", {"url": auth.api_url, "module": payload.get("module")})
            try:
                response = requests.post(auth.api_url, data=payload, timeout=self.request_timeout)
                response.raise_for_status()
                res = response.json()

                response_details = {"url": auth.api_url, "module": payload.get("module"), "status": res.get("status")}
                if res.get("status") != "SUCCESS":
                    if res.get("message"):
                        response_details["error_message"] = res.get("message")
                    if isinstance(res.get("data"), dict) and res.get("data", {}).get("description"):
                        response_details["error_description"] = res.get("data").get("description")
                    elif isinstance(res.get("data"), str):
                        response_details["error_data_string"] = res.get("data")
                self.logger.emit("api_response", response_details)

            except requests.exceptions.Timeout as e:
                self.logger.emit("website_unresponsive", {"url": auth.api_url, "error": f"Timeout: {str(e)}"})
                return "UNRESPONSIVE"
            except requests.exceptions.ConnectionError as e:
                self.logger.emit("website_unresponsive", {"url": auth.api_url, "error": f"ConnectionError: {str(e)}"})
                return "UNRESPONSIVE"
            except Exception as e: # This includes JSONDecodeError if response is not JSON
                self.logger.emit("exception", {"error": f"Downline fetch failed for {auth.api_url}: {str(e)}"})
                return "ERROR"

            if res.get("status") != "SUCCESS":
                return "ERROR"

            new_rows: List[Downline] = []
            for d in res["data"].get("downlines", []):
                row = Downline(
                    url=url,
                    id=d.get("id"),
                    name=d.get("name"),
                    count=d.get("count", 0),
                    amount=float(d.get("amount", 0) or 0),
                    register_date_time=d.get("registerDateTime")
                )
                key = (
                    str(row.url), str(row.id), str(row.name),
                    str(row.count), str(row.amount), str(row.register_date_time)
                )
                if key not in written:
                    new_rows.append(row)
                    written.add(key)

            if not new_rows:
                break

            file_exists_and_not_empty = os.path.exists(csv_file) and os.path.getsize(csv_file) > 0
            with open(csv_file, "a", newline="", encoding="utf-8") as f:
                fieldnames = [field.name for field in Downline.__dataclass_fields__.values()]
                writer = csv.DictWriter(f, fieldnames=fieldnames)
                if not file_exists_and_not_empty:
                    writer.writeheader()
                writer.writerows([row.__dict__ for row in new_rows])
                self.logger.emit("csv_written", {"file": csv_file, "count": len(new_rows)})

            total_new_rows += len(new_rows)
            page += 1

        self.logger.emit("downline_fetched", {"count": total_new_rows})
        return total_new_rows

    def fetch_bonuses(self, url: str, auth: AuthData, csv_file: str = "bonuses.csv") -> Union[Tuple[int, float, dict[str, bool]], str]:
        C_KEYWORDS = ["commission", "affiliate"]
        D_KEYWORDS = ["downline first deposit"]
        S_KEYWORDS = ["share bonus", "referrer"]
        bonus_type_flags = {"C": False, "D": False, "S": False, "O": False}

        payload = {
            "module": "/users/syncData", "merchantId": auth.merchant_id, "domainId": "0",
            "accessId": auth.access_id, "accessToken": auth.token, "walletIsAdmin": ""
        }
        self.logger.emit("api_request", {"url": auth.api_url, "module": payload.get("module")})
        try:
            response = requests.post(auth.api_url, data=payload, timeout=self.request_timeout)
            response.raise_for_status()
            res = response.json()
            response_details = {"url": auth.api_url, "module": payload.get("module"), "status": res.get("status")}
            if res.get("status") != "SUCCESS":
                if res.get("message"): response_details["error_message"] = res.get("message")
                if isinstance(res.get("data"), dict) and res.get("data", {}).get("description"):
                    response_details["error_description"] = res.get("data").get("description")
                elif isinstance(res.get("data"), str): response_details["error_data_string"] = res.get("data")
            self.logger.emit("api_response", response_details)
        except requests.exceptions.Timeout as e:
            self.logger.emit("website_unresponsive", {"url": auth.api_url, "error": f"Timeout: {str(e)}"})
            return "UNRESPONSIVE"
        except requests.exceptions.ConnectionError as e:
            self.logger.emit("website_unresponsive", {"url": auth.api_url, "error": f"ConnectionError: {str(e)}"})
            return "UNRESPONSIVE"
        except Exception as e:
            self.logger.emit("exception", {"error": f"Bonus fetch failed for {auth.api_url}: {str(e)}"})
            return "ERROR"

        if res.get("status") != "SUCCESS":
            self.logger.emit("bonus_api_error", {"url": auth.api_url, "status": res.get("status"), "error_message": res.get("message", "N/A"), "error_data": res.get("data", "N/A")})
            return "ERROR"

        bonuses_data_raw = res.get("data", {}).get("bonus", []) + res.get("data", {}).get("promotions", [])
        if not bonuses_data_raw:
            self.logger.emit("bonus_fetched", {"count": 0, "total_amount": 0.0})
            return 0, 0.0, bonus_type_flags

        rows_to_write_obj: List[Bonus] = []
        os.makedirs(os.path.dirname(csv_file), exist_ok=True)
        file_exists_and_not_empty = os.path.exists(csv_file) and os.path.getsize(csv_file) > 0
        with open(csv_file, "a", newline="", encoding="utf-8") as f:
            fieldnames = [field.name for field in Bonus.__dataclass_fields__.values()]
            writer = csv.DictWriter(f, fieldnames=fieldnames)
            if not file_exists_and_not_empty:
                writer.writeheader()

            for b_data in bonuses_data_raw:
                try:
                    min_w = float(b_data.get("minWithdraw", 0) or 0)
                    bonus_f = float(b_data.get("bonusFixed", 0) or 0)
                    ratio = min_w / bonus_f if bonus_f != 0 else None
                except (ValueError, TypeError):
                    self.logger.emit("exception", {"error": f"Type error processing bonus data for {url}: {b_data}"})
                    continue

                bonus_instance = Bonus(
                    url=url, merchant_name=auth.merchant_name, id=b_data.get("id"), name=b_data.get("name"),
                    transaction_type=b_data.get("transactionType"), bonus_fixed=float(b_data.get("bonusFixed", 0) or 0),
                    amount=float(b_data.get("amount", 0) or 0), min_withdraw=float(b_data.get("minWithdraw", 0) or 0),
                    max_withdraw=float(b_data.get("maxWithdraw", 0) or 0), withdraw_to_bonus_ratio=ratio,
                    rollover=float(b_data.get("rollover", 0) or 0), balance=str(b_data.get("balance", "")),
                    claim_config=str(b_data.get("claimConfig", "")), claim_condition=str(b_data.get("claimCondition", "")),
                    bonus=str(b_data.get("bonus", "")), bonus_random=str(b_data.get("bonusRandom", "")),
                    reset=str(b_data.get("reset", "")), min_topup=float(b_data.get("minTopup", 0) or 0),
                    max_topup=float(b_data.get("maxTopup", 0) or 0), refer_link=str(b_data.get("referLink", ""))
                )
                rows_to_write_obj.append(bonus_instance)

                name_lower = bonus_instance.name.lower() if bonus_instance.name else ""
                claim_config_lower = bonus_instance.claim_config.lower() if bonus_instance.claim_config else ""
                matched_c_d_s_for_this_bonus = False
                if any(keyword in name_lower or keyword in claim_config_lower for keyword in C_KEYWORDS):
                    bonus_type_flags["C"] = True; matched_c_d_s_for_this_bonus = True
                if any(keyword in name_lower or keyword in claim_config_lower for keyword in D_KEYWORDS):
                    bonus_type_flags["D"] = True; matched_c_d_s_for_this_bonus = True
                if any(keyword in name_lower or keyword in claim_config_lower for keyword in S_KEYWORDS):
                    bonus_type_flags["S"] = True; matched_c_d_s_for_this_bonus = True
                if not matched_c_d_s_for_this_bonus:
                    bonus_type_flags["O"] = True
            if rows_to_write_obj:
                writer.writerows([b.__dict__ for b in rows_to_write_obj])
                self.logger.emit("csv_written", {"file": csv_file, "count": len(rows_to_write_obj)})

        current_fetch_total_amount = sum(b.amount for b in rows_to_write_obj)
        self.logger.emit("bonus_fetched", {"count": len(rows_to_write_obj), "total_amount": current_fetch_total_amount})
        return len(rows_to_write_obj), current_fetch_total_amount, bonus_type_flags

def load_urls(url_file: str) -> List[str]:
    if not os.path.exists(url_file):
        print(f"URL file not found: {url_file}")
        return []
    with open(url_file, "r") as f:
        return [url.strip() for url in f if url.strip()]

def main():
    config_loader = ConfigLoader(path="config.ini")
    config = config_loader.load()
    run_cache_data = load_run_cache()
    run_cache_data["total_script_runs"] += 1
    REQUEST_TIMEOUT = 30
    unresponsive_sites_this_run = []
    logger = Logger(log_file=config.logging.log_file, log_level=config.logging.log_level, console=config.logging.console, detail=config.logging.detail)

    # Initialize GoogleSheetsUploader
    gs_uploader = None
    if config.google_sheets.enabled:
        gs_uploader = GoogleSheetsUploader(config.google_sheets, logger)
    else:
        logger.emit("google_sheets_info", {"message": "Google Sheets uploading is disabled in config."})

    auth_service = AuthService(logger)
    scraper = Scraper(logger, REQUEST_TIMEOUT)
    urls = load_urls(config.settings.url_file)

    def format_stat_display(current_val, prev_val):
        if current_val == 0 and prev_val == 0: return ""
        diff = current_val - prev_val
        return f"{current_val}/{prev_val}({diff:+})"

    if not urls:
        logger.emit("job_start", {"url_count": 0, "status": "No URLs to process"})
        print("No URLs to process. Exiting.")
        return

    total_urls = len(urls)
    history = logger.load_metrics(config.logging.log_file)
    metrics = {
        "bonuses_old": history.get("bonuses", 0), "downlines_old": history.get("downlines", 0),
        "errors_old": history.get("errors", 0), "bonuses_new": 0, "downlines_new": 0, "errors_new": 0,
        "bonus_amount_new": 0.0
    }
    metrics["bonuses_total_old"] = history.get("bonuses", 0)
    metrics["downlines_total_old"] = history.get("downlines", 0)
    metrics["errors_total_old"] = history.get("errors", 0)
    metrics["bonus_amount_total_old"] = history.get("total_bonus_amount", 0.0)
    metrics["bonuses_total_new"] = metrics["bonuses_total_old"]
    metrics["downlines_total_new"] = metrics["downlines_total_old"]
    metrics["errors_total_new"] = metrics["errors_total_old"]
    metrics["bonus_amount_total_new"] = metrics["bonus_amount_total_old"]

    try:
        logger.emit("job_start", {"url_count": total_urls, "total_script_runs": run_cache_data.get("total_script_runs", "N/A")})
        start_time = time.time()

        for idx, url in enumerate(urls, 1):
            if idx > 1:
                sys.stdout.write('\x1b[3A')
                sys.stdout.write('\x1b[J')

            site_start_time = time.time()
            cleaned_url = auth_service.clean_url(url)
            site_key = cleaned_url
            cr_bonuses_site, cr_downlines_site, cr_errors_site = 0, 0, 0
            site_cache_entry = run_cache_data["sites"].get(site_key, {})
            pr_bonuses = site_cache_entry.get("last_run_new_bonuses", 0)
            prt_bonuses = site_cache_entry.get("cumulative_total_bonuses", 0)
            pr_downlines = site_cache_entry.get("last_run_new_downlines", 0)
            prt_downlines = site_cache_entry.get("cumulative_total_downlines", 0)
            pr_errors = site_cache_entry.get("last_run_new_errors", 0)
            prt_errors = site_cache_entry.get("cumulative_total_errors", 0)

            try:
                auth_data = auth_service.login(cleaned_url, config.credentials.mobile, config.credentials.password)
                current_site_bonus_flags = {"C": False, "D": False, "S": False, "O": False}
                if not auth_data:
                    metrics["errors_new"] += 1; metrics["errors_total_new"] += 1; cr_errors_site = 1
                    logger.emit("exception", {"error": f"Authentication failed for {cleaned_url}"})

                if auth_data:
                    if config.settings.downline_enabled:
                        result_dl = scraper.fetch_downlines(cleaned_url, auth_data)
                        if isinstance(result_dl, str):
                            metrics["errors_new"] += 1; metrics["errors_total_new"] += 1; cr_errors_site = 1
                            if result_dl == "UNRESPONSIVE": unresponsive_sites_this_run.append(cleaned_url)
                        else:
                            cr_downlines_site = result_dl
                            metrics["downlines_new"] += result_dl; metrics["downlines_total_new"] += result_dl
                    else:
                        bonus_csv_path = datetime.now().strftime("data/%m-%d bonuses.csv")
                        result_bonuses = scraper.fetch_bonuses(cleaned_url, auth_data, csv_file=bonus_csv_path)
                        if isinstance(result_bonuses, str):
                            metrics["errors_new"] += 1; metrics["errors_total_new"] += 1; cr_errors_site = 1
                            if result_bonuses == "UNRESPONSIVE": unresponsive_sites_this_run.append(cleaned_url)
                        else:
                            count, current_fetch_total_amount, current_site_bonus_flags = result_bonuses
                            cr_bonuses_site = count
                            metrics["bonuses_new"] += count; metrics["bonus_amount_new"] += current_fetch_total_amount
                            metrics["bonuses_total_new"] += count; metrics["bonus_amount_total_new"] += current_fetch_total_amount
            except Exception as e:
                metrics["errors_new"] += 1; metrics["errors_total_new"] += 1; cr_errors_site = 1
                logger.emit("exception", {"error": f"Outer loop exception for {cleaned_url}: {str(e)}"})

            crt_bonuses = prt_bonuses + cr_bonuses_site
            crt_downlines = prt_downlines + cr_downlines_site
            crt_errors = prt_errors + cr_errors_site
            run_cache_data["sites"].setdefault(site_key, {})
            run_cache_data["sites"][site_key].update({
                "last_run_new_bonuses": cr_bonuses_site, "cumulative_total_bonuses": crt_bonuses,
                "last_run_new_downlines": cr_downlines_site, "cumulative_total_downlines": crt_downlines,
                "last_run_new_errors": cr_errors_site, "cumulative_total_errors": crt_errors,
                "bonus_flags": current_site_bonus_flags
            })

            site_processing_duration = time.time() - site_start_time
            percent = (idx / total_urls) * 100
            run_count = run_cache_data["total_script_runs"]
            sfs = run_cache_data["sites"][site_key] # Use the newly updated cache entry for display stats

            bonus_flags = sfs.get('bonus_flags', {})
            flags_str = f"[C] {'Y' if bonus_flags.get('C') else 'N'} [D] {'Y' if bonus_flags.get('D') else 'N'} [S] {'Y' if bonus_flags.get('S') else 'N'} [O] {'Y' if bonus_flags.get('O') else 'N'}"
            progress_bar_str = progress(idx, vmin=0, vmax=total_urls, length=40, title="")

            line1 = f"| {progress_bar_str} | [{percent:.2f}%] {idx}/{total_urls} |"
            line2 = f"| {site_processing_duration:.1f}s | [Run #{run_count}] | {flags_str} | [URL] {cleaned_url} |"

            r_b = format_stat_display(sfs['last_run_new_bonuses'], pr_bonuses) # Use pr_bonuses for prev val
            t_b = format_stat_display(sfs['cumulative_total_bonuses'], prt_bonuses) # Use prt_bonuses for prev val
            stats_b_str = f"[B]|[R]:{r_b if r_b else '-'} [T]:{t_b if t_b else '-'}"
            stats_d_str = ""
            if config.settings.downline_enabled:
                r_d = format_stat_display(sfs['last_run_new_downlines'], pr_downlines)
                t_d = format_stat_display(sfs['cumulative_total_downlines'], prt_downlines)
                stats_d_str = f"| [D]|[R]:{r_d if r_d else '-'} [T]:{t_d if t_d else '-'}"
            r_e = format_stat_display(sfs['last_run_new_errors'], pr_errors)
            t_e = format_stat_display(sfs['cumulative_total_errors'], prt_errors)
            stats_e_str = f"| [E]|[R]:{r_e if r_e else '-'} [T]:{t_e if t_e else '-'}"
            line3 = f"| {stats_b_str} {stats_d_str} {stats_e_str} |"

            sys.stdout.write(f"{line1}\n{line2}\n{line3}\n"); sys.stdout.flush()

        # This block is now correctly indented
        elapsed = time.time() - start_time
        sys.stdout.write("\n")

        avg_bonus_amount_this_run = (metrics["bonus_amount_new"] / metrics["bonuses_new"]) if metrics["bonuses_new"] > 0 else 0.0
        job_summary_details = {
            "duration": elapsed, "total_urls_processed": total_urls,
            "bonuses_fetched_this_run": metrics["bonuses_new"], "bonus_amount_this_run": metrics["bonus_amount_new"],
            "avg_bonus_amount_this_run": avg_bonus_amount_this_run, "downlines_fetched_this_run": metrics["downlines_new"],
            "errors_this_run": metrics["errors_new"], "unresponsive_sites_count_this_run": len(unresponsive_sites_this_run)
        }

        today_date_str = datetime.now().strftime('%m-%d')
        daily_bonus_csv_path = f"data/{today_date_str} bonuses.csv"
        historical_excel_path = "data/historical_bonuses.xlsx"
        if not config.settings.downline_enabled:
            if os.path.exists(daily_bonus_csv_path) and os.path.getsize(daily_bonus_csv_path) > 0:
                try:
                    bonus_df_for_excel = pd.read_csv(daily_bonus_csv_path) # Renamed to avoid conflict
                    if not bonus_df_for_excel.empty:
                        os.makedirs(os.path.dirname(historical_excel_path), exist_ok=True)
                        mode = 'a' if os.path.exists(historical_excel_path) else 'w'
                        with pd.ExcelWriter(historical_excel_path, engine='openpyxl', mode=mode, if_sheet_exists='replace') as writer:
                            bonus_df_for_excel.to_excel(writer, sheet_name=today_date_str, index=False)
                        logger.emit("historical_data_written", {"file": historical_excel_path, "sheet": today_date_str, "rows": len(bonus_df_for_excel)})
                    else:
                        logger.emit("historical_data_skipped", {"reason": "Daily bonus CSV is empty", "file": daily_bonus_csv_path})
                except Exception as e:
                    logger.emit("historical_data_error", {"file": daily_bonus_csv_path, "excel_file": historical_excel_path, "error": str(e)})
            else:
                logger.emit("historical_data_skipped", {"reason": "Daily bonus CSV not found or empty", "file": daily_bonus_csv_path})

            # Upload Daily Bonus CSV to Google Sheets
            if gs_uploader and config.google_sheets.upload_daily_bonus and os.path.exists(daily_bonus_csv_path) and os.path.getsize(daily_bonus_csv_path) > 0:
                try:
                    daily_bonus_df = pd.read_csv(daily_bonus_csv_path) # Read the CSV that was just saved
                    if not daily_bonus_df.empty:
                        sheet_name = f"DailyBonus_{datetime.now().strftime('%Y-%m-%d')}"
                        logger.emit("google_sheets_upload_start", {"file": daily_bonus_csv_path, "sheet_name": sheet_name})
                        gs_uploader.upload_dataframe(daily_bonus_df, sheet_name)
                    else:
                        logger.emit("google_sheets_upload_skipped", {"reason": "Daily bonus CSV is empty, skipping upload.", "file": daily_bonus_csv_path})
                except Exception as e:
                    logger.emit("google_sheets_upload_error", {"file": daily_bonus_csv_path, "error": f"Failed to prepare or upload daily bonus data: {str(e)}"})
            elif gs_uploader and config.google_sheets.upload_daily_bonus: # Condition to log if upload was enabled but file was missing/empty
                 logger.emit("google_sheets_upload_skipped", {"reason": "Daily bonus CSV not found or empty, skipping upload.", "file": daily_bonus_csv_path})

            # Upload Historical Bonus (Latest Sheet) to Google Sheets
            if gs_uploader and config.google_sheets.upload_historical_bonus and 'bonus_df_for_excel' in locals() and not bonus_df_for_excel.empty:
                try:
                    # Use today's date for the sheet name, similar to how it's done for Excel
                    sheet_name = datetime.now().strftime('%m-%d') # This matches the Excel sheet name
                    logger.emit("google_sheets_upload_start", {"source": "historical_bonus_df", "sheet_name": sheet_name})
                    gs_uploader.upload_dataframe(bonus_df_for_excel, sheet_name) # bonus_df_for_excel is the DataFrame used for the Excel sheet
                except Exception as e:
                    logger.emit("google_sheets_upload_error", {"source": "historical_bonus_df", "sheet_name": sheet_name, "error": f"Failed to upload historical bonus data: {str(e)}"})
            elif gs_uploader and config.google_sheets.upload_historical_bonus: # Condition to log if upload was enabled but df was missing/empty
                 logger.emit("google_sheets_upload_skipped", {"reason": "Historical bonus dataframe (bonus_df_for_excel) not available or empty."})

        try:
            today_dt = datetime.now()
            yesterday_dt = today_dt - timedelta(days=1)
            today_sheet_name_comp = today_dt.strftime('%m-%d') # Renamed
            yesterday_sheet_name_comp = yesterday_dt.strftime('%m-%d') # Renamed
            comparison_report_path = f"data/comparison_report_{today_sheet_name_comp}.csv"

            today_df_comp = None # Renamed
            # Use daily_bonus_csv_path if it was for today and bonus_df_for_excel is available
            if 'bonus_df_for_excel' in locals() and isinstance(bonus_df_for_excel, pd.DataFrame) and not bonus_df_for_excel.empty and \
               daily_bonus_csv_path == f"data/{today_sheet_name_comp} bonuses.csv":
                 today_df_comp = bonus_df_for_excel

            if today_df_comp is None:
                current_day_bonus_csv = f"data/{today_sheet_name_comp} bonuses.csv"
                if os.path.exists(current_day_bonus_csv) and os.path.getsize(current_day_bonus_csv) > 0:
                    today_df_comp = pd.read_csv(current_day_bonus_csv)
                else:
                    today_df_comp = pd.DataFrame()

            yesterday_df_comp = pd.DataFrame() # Renamed
            if os.path.exists(historical_excel_path):
                try:
                    yesterday_df_comp = pd.read_excel(historical_excel_path, sheet_name=yesterday_sheet_name_comp)
                except Exception as e: # Simplified error handling for brevity in this section
                    logger.emit("comparison_info", {"message": f"Could not read yesterday's sheet ({yesterday_sheet_name_comp}) for comparison: {str(e)}"})

            expected_columns = [
                'url', 'merchant_name', 'id', 'name', 'transaction_type', 'bonus_fixed', 'amount',
                'min_withdraw', 'max_withdraw', 'withdraw_to_bonus_ratio', 'rollover', 'balance',
                'claim_config', 'claim_condition', 'bonus', 'bonus_random', 'reset',
                'min_topup', 'max_topup', 'refer_link'
            ]

            if today_df_comp.empty: today_df_comp = pd.DataFrame(columns=expected_columns)
            else:
                for col in expected_columns:
                    if col not in today_df_comp.columns: today_df_comp[col] = pd.NA
            if yesterday_df_comp.empty: yesterday_df_comp = pd.DataFrame(columns=expected_columns)
            else:
                for col in expected_columns:
                    if col not in yesterday_df_comp.columns: yesterday_df_comp[col] = pd.NA

            key_cols = ['merchant_name', 'name', 'amount']
            for df_ref in [today_df_comp, yesterday_df_comp]:
                if not df_ref.empty:
                    for col in key_cols:
                        if col == 'amount': df_ref[col] = pd.to_numeric(df_ref[col], errors='coerce').round(5)
                        else: df_ref[col] = df_ref[col].astype(str).fillna('')
                    df_ref.dropna(subset=[k for k in key_cols if k in df_ref.columns], how='any', inplace=True)

            if not today_df_comp.empty: today_df_comp['_comparison_key'] = today_df_comp.apply(lambda row: f"{row['merchant_name']}_{row['name']}_{row['amount']}", axis=1)
            else: today_df_comp['_comparison_key'] = pd.Series(dtype='object')
            if not yesterday_df_comp.empty: yesterday_df_comp['_comparison_key'] = yesterday_df_comp.apply(lambda row: f"{row['merchant_name']}_{row['name']}_{row['amount']}", axis=1)
            else: yesterday_df_comp['_comparison_key'] = pd.Series(dtype='object')

            report_data_list = []
            if not today_df_comp.empty or not yesterday_df_comp.empty: # Proceed if at least one DF has data
                merged_df = pd.merge(today_df_comp, yesterday_df_comp, on='_comparison_key', how='outer', suffixes=('_today', '_yesterday'), indicator=True)
                report_columns = ['status', 'change_details'] + expected_columns
                for _, row in merged_df.iterrows(): # Renamed idx to _ to avoid clash with outer loop
                    item_details, status, change_details_str = {}, "", ""
                    is_new, is_used, is_persistent = row['_merge'] == 'left_only', row['_merge'] == 'right_only', row['_merge'] == 'both'

                    if is_new: status = "New"; suffix = '_today'
                    elif is_used: status = "Used"; suffix = '_yesterday'
                    else: status = "Persistent_Unchanged"; suffix = '_today' # Default for persistent

                    for col in expected_columns: item_details[col] = row.get(col + suffix, pd.NA)

                    if is_persistent:
                        changes = []
                        for col in expected_columns:
                            val_t, val_y = row.get(col + '_today'), row.get(col + '_yesterday')
                            if pd.isna(val_t) and pd.isna(val_y): continue
                            if pd.isna(val_t) or pd.isna(val_y) or str(val_t) != str(val_y):
                                if isinstance(val_t, float) or isinstance(val_y, float): # Numerical comparison
                                    if round(pd.to_numeric(val_t, errors='coerce'),5) != round(pd.to_numeric(val_y, errors='coerce'),5):
                                        changes.append(f"{col}: '{val_y}' -> '{val_t}'")
                                else: # String comparison
                                    changes.append(f"{col}: '{val_y}' -> '{val_t}'")
                        if changes: status = "Persistent_Changed"; change_details_str = "; ".join(changes)

                    item_details['status'], item_details['change_details'] = status, change_details_str
                    report_data_list.append({key: item_details.get(key) for key in report_columns})

                if report_data_list:
                    report_df = pd.DataFrame(report_data_list, columns=report_columns)
                    os.makedirs(os.path.dirname(comparison_report_path), exist_ok=True)
                    report_df.to_csv(comparison_report_path, index=False, encoding='utf-8')
                    logger.emit("comparison_report_generated", {"path": comparison_report_path, "rows": len(report_df)})

                    # Upload Comparison Report CSV to Google Sheets
                    if gs_uploader and config.google_sheets.upload_comparison_report and 'report_df' in locals() and not report_df.empty:
                        try:
                            sheet_name = f"ComparisonReport_{datetime.now().strftime('%Y-%m-%d')}"
                            logger.emit("google_sheets_upload_start", {"file": comparison_report_path, "sheet_name": sheet_name})
                            gs_uploader.upload_dataframe(report_df, sheet_name)
                        except Exception as e:
                            logger.emit("google_sheets_upload_error", {"file": comparison_report_path, "error": f"Failed to upload comparison report: {str(e)}"})
                    elif gs_uploader and config.google_sheets.upload_comparison_report: # Log if upload enabled but df missing/empty
                        logger.emit("google_sheets_upload_skipped", {"reason": "Comparison report dataframe (report_df) not available or empty."})
                else:
                    logger.emit("comparison_info", {"message": "No changes for comparison report."})
            else:
                logger.emit("comparison_info", {"message": "Both today's and yesterday's bonus data are empty. No comparison report generated."})
        except Exception as e:
            import traceback
            logger.emit("comparison_module_error", {"error_type": type(e).__name__, "error": str(e), "traceback": traceback.format_exc()})

        logger.emit("job_complete", job_summary_details)
        if unresponsive_sites_this_run:
            logger.emit("down_sites_summary", {"sites": unresponsive_sites_this_run, "count": len(unresponsive_sites_this_run)})
    finally:
        save_run_cache(run_cache_data)
        logger.emit("cache_saved", {"path": "data/run_metrics_cache.json", "total_script_runs": run_cache_data.get("total_script_runs")})

if __name__ == "__main__":
    main()
--- END OF FILE: src/main.py ---
--- START OF FILE: src/models.py ---
from dataclasses import dataclass
from typing import Optional

@dataclass
class AuthData:
    merchant_id: str
    merchant_name: str
    access_id: str
    token: str
    api_url: str

@dataclass
class Downline:
    url: str
    id: str
    name: str
    count: int
    amount: float
    register_date_time: str

@dataclass
class Bonus:
    url: str
    merchant_name: str
    id: str
    name: str
    transaction_type: str
    bonus_fixed: float
    amount: float
    min_withdraw: float
    max_withdraw: float
    withdraw_to_bonus_ratio: Optional[float]
    rollover: float
    balance: str
    claim_config: str
    claim_condition: str
    bonus: str
    bonus_random: str
    reset: str
    min_topup: float
    max_topup: float
    refer_link: str
--- END OF FILE: src/models.py ---
--- START OF FILE: src/scraper.kv ---
#:kivy 2.0.0

# Define Colors and Fonts
<colors@Widget>: # Use a dummy class or existing class for constants if preferred
    primary_bg: (0.133, 0.133, 0.133, 1) # Dark gray #222222
    secondary_bg: (0.2, 0.2, 0.2, 1)    # Slightly lighter dark gray #333333
    primary_text: (0.878, 0.878, 0.878, 1) # Light gray / Off-white #E0E0E0
    highlight: (1, 0.2, 0.2, 1)          # Red #FF3333
    highlight_text: (0.898, 0.451, 0.451, 1) # Red for text #E57373
    disabled_button_bg: (0.25, 0.25, 0.25, 1)
    disabled_button_text: (0.5, 0.5, 0.5, 1)

# Global font settings
<Widget>:
    font_name: 'Roboto'
    font_size: '14sp'

# Base Screen styling
<Screen>:
    background_color: colors.primary_bg

<Label>:
    color: colors.primary_text
    markup: True
    # Default padding for labels in config screen for better alignment
    # padding_x: '5dp' # This might be too global, prefer to set on specific labels if needed

<ConfigScreenLabel@Label>: # Specific labels in ConfigScreen if needed
    size_hint_x: 0.4
    halign: 'left'
    valign: 'center'
    text_size: self.width, None # For wrapping

<Button>:
    color: colors.primary_text
    background_color: colors.secondary_bg
    background_normal: ''
    background_down: colors.highlight # Use highlight color for press
    border: (2, 2, 2, 2)
    size_hint_y: None
    height: '44dp' # Default button height
    disabled_background_color: colors.disabled_button_bg
    disabled_color: colors.disabled_button_text
    on_disabled:
        # Explicitly set background and color when disabled, as Kivy's default might not use disabled_background_color always
        if self.disabled: \
        self.background_color = colors.disabled_button_bg; \
        self.color = colors.disabled_button_text
        else: \
        self.background_color = colors.secondary_bg; \
        self.color = colors.primary_text


<NavButton@Button>:
    is_active: False
    background_color: colors.highlight if self.is_active else colors.secondary_bg
    on_is_active:
        # Ensure color updates if is_active changes AFTER initial on_disabled might have run
        if self.disabled: return # Don't override disabled style
        self.background_color = colors.highlight if self.is_active else colors.secondary_bg


<TextInput>:
    background_color: colors.primary_bg  # Darker background for input area
    foreground_color: colors.primary_text
    cursor_color: colors.highlight_text
    selection_color: colors.highlight_text
    padding: [10, 10, 10, 10]
    # Use a slightly lighter border or different background for the text input itself
    # canvas.before:
    #     Color:
    #         rgba: colors.secondary_bg # Border color
    #     BorderImage:
    #         border: (2,2,2,2) # Thicker border for inputs
    #         source: 'atlas://data/images/defaulttheme/textinput' # Kivy's default, but with our border color
    #     Color:
    #         rgba: colors.primary_bg # Background for text area
    #     Rectangle:
    #         pos: self.pos[0]+1, self.pos[1]+1
    #         size: self.size[0]-2, self.size[1]-2


<CheckBox>:
    color: colors.highlight # Color of the check mark itself
    # For the box, it's tricky. Kivy's default is simple.
    # We can try to use canvas instructions for a custom look if needed,
    # but it adds complexity. For now, just the checkmark color.

<ScrollView>:
    bar_color: colors.highlight
    bar_inactive_color: colors.secondary_bg
    bar_width: '6dp' # Slightly thinner
    scroll_type: ['bars', 'content']

<BoxLayout>: # Default for general BoxLayouts
    padding: '10dp'
    spacing: '5dp'

<GridLayout>: # Default for general GridLayouts
    padding: '10dp'
    spacing: '5dp'

# ConfigScreen specific styles
# ConfigScreen's GridLayout is created in Python. We can style its children.
# Labels within ConfigScreen's GridLayout will get <Label> style.
# TextInputs will get <TextInput> style.
# The status label and save button in ConfigScreen are named `self.config_status_label` and `self.save_button`
# They will get global styles. If specific styling is needed, assign them a class.
# Example for the status label in ConfigScreen:
# <ConfigStatusLabel@Label>:
#     id: config_status_label # This id is for Python access if created in KV
#     # Custom styles here

# ProgressScreen - Structure defined here
<ProgressScreen>:
    BoxLayout:
        orientation: 'vertical'
        # padding and spacing from global BoxLayout rule, or override here
        # padding: '10dp'
        # spacing: '10dp' # From previous KV
        Button:
            id: start_button
            text: 'Start Scraping'
            # size_hint_y, height from global Button rule
            on_press: app.start_scraping_thread(self) # Pass the button instance
        Label:
            id: status_label
            size_hint_y: None
            height: '30dp' # Or use sp if text size dictates height
            text: "" # Initial text
        ScrollView:
            id: log_display_scroll
            TextInput:
                id: log_display
                readonly: True
                multiline: True
                size_hint_y: None
                height: self.minimum_height
                font_name: 'RobotoMono-Regular' # Monospaced for logs
                # background_color from global TextInput
                # foreground_color from global TextInput

# HistoryScreen - Structure defined here
<HistoryScreen>:
    BoxLayout:
        orientation: 'vertical'
        # padding: '10dp'
        # spacing: '10dp' # From previous KV
        ScrollView:
            id: metrics_scrollview # Added id for clarity
            GridLayout:
                id: metrics_layout
                cols: 2
                size_hint_y: None
                height: self.minimum_height
                # Labels inside will get global <Label> style.
                # Specific styling for these labels can be done by giving them a class.
                # e.g. <MetricNameLabel@Label>: ... <MetricValueLabel@Label>: ...
        Button:
            id: refresh_button
            text: 'Refresh Data'
            # size_hint_y, height from global Button rule
            on_press: root.load_and_display_metrics() # root refers to HistoryScreen instance

# Root layout styling (ScraperApp's root BoxLayout)
# If ScraperApp's root_layout in Python was `self.root_layout = BoxLayout(...)`,
# we could do `<RootAppLayout@BoxLayout>:` if we assigned `self.root_layout.name = 'RootAppLayout'`
# For now, the nav_bar and ScreenManager are added to a standard BoxLayout.
# The Nav Bar itself (BoxLayout)
# <NavBarLayout@BoxLayout>: # If self.nav_bar in Python gets this class
#    size_hint_y: None
#    height: '50dp' # Already set in Python, but KV can override if class is applied
#    padding: '5dp' # Example custom padding for nav bar
#    spacing: '5dp' # Example custom spacing
# NavButtons inside it will get <NavButton> style.
--- END OF FILE: src/scraper.kv ---
--- START OF FILE: src/utils.py ---
import sys
import math
import json
import os

CACHE_FILE_PATH = "data/run_metrics_cache.json"

def load_run_cache():
    """
    Loads run metrics cache from a JSON file.
    Returns default structure if file not found or JSON is invalid.
    """
    default_cache = {"total_script_runs": 0, "sites": {}}
    if not os.path.exists(CACHE_FILE_PATH):
        print(f"Info: Cache file '{CACHE_FILE_PATH}' not found. Returning default cache.")
        return default_cache
    try:
        with open(CACHE_FILE_PATH, 'r') as f:
            data = json.load(f)
            # Basic validation for expected top-level keys
            if "total_script_runs" not in data or "sites" not in data:
                print(f"Warning: Cache file '{CACHE_FILE_PATH}' is missing expected keys. Returning default cache.")
                return default_cache
            return data
    except FileNotFoundError: # Should be caught by os.path.exists, but good for robustness
        print(f"Info: Cache file '{CACHE_FILE_PATH}' not found (FileNotFoundError). Returning default cache.")
        return default_cache
    except json.JSONDecodeError:
        print(f"Warning: Cache file '{CACHE_FILE_PATH}' contains invalid JSON. Returning default cache.")
        return default_cache
    except Exception as e:
        print(f"Warning: An unexpected error occurred while loading cache file '{CACHE_FILE_PATH}': {e}. Returning default cache.")
        return default_cache

def save_run_cache(data):
    """
    Saves run metrics cache to a JSON file.
    Ensures the directory exists.
    """
    try:
        os.makedirs(os.path.dirname(CACHE_FILE_PATH), exist_ok=True)
        with open(CACHE_FILE_PATH, 'w') as f:
            json.dump(data, f, indent=4)
    except Exception as e:
        # In a real app, this might go to a logger if available
        print(f"Error: Could not save cache file '{CACHE_FILE_PATH}': {e}")


def progress(value, length=40, title=" ", vmin=0.0, vmax=1.0):
    """
    Text progress bar

    Parameters
    ----------
    value : float
        Current value to be displayed as progress
    vmin : float
        Minimum value
    vmax : float
        Maximum value
    length: int
        Bar length (in character)
    title: string
        Text to be prepend to the bar
    """
    # Block progression is 1/8
    blocks = ["", "▏","▎","▍","▌","▋","▊","▉","█"]
    vmin = vmin or 0.0
    vmax = vmax or 1.0
    lsep, rsep = "▏", "▕" # Changed from "▏", "▕" to avoid potential rendering issues with some fonts for lsep. Using simple pipe.

    # Normalize value
    value = min(max(value, vmin), vmax)
    value = (value - vmin) / float(vmax - vmin) if (vmax - vmin) != 0 else 0.0 # Avoid division by zero

    v = value * length
    x = math.floor(v)  # integer part
    y = v - x  # fractional part
    base = 0.125  # 0.125 = 1/8
    prec = 3
    # Ensure index i is within the bounds of the blocks list
    i = min(len(blocks) - 1, int(round(base * math.floor(float(y) / base), prec) / base if base != 0 else 0)) # Avoid division by zero

    bar = "█" * x + blocks[i]
    n = length - len(bar)
    bar = lsep + bar + " " * n + rsep

    # Prepare the string but do not print it directly.
    # The caller in main.py will handle the actual printing.
    # This makes the utility function more flexible.
    # Return the formatted string instead of printing.
    return f"{title}{bar} {value*100:.1f}%"

if __name__ == '__main__':
    import time

    for i in range(1001): # Test up to 1000
        # progress_str = progress(i, vmin=0, vmax=1000, length=50, title="Test Progress: ")
        # sys.stdout.write("\r" + progress_str)
        # sys.stdout.flush()
        # The function now returns the string, so main would do:
        sys.stdout.write("\r" + progress(i, vmin=0, vmax=1000, length=50, title="Test Progress: "))
        sys.stdout.flush()
        time.sleep(0.0025)
    sys.stdout.write("\n")
--- END OF FILE: src/utils.py ---
--- START OF FILE: urls.txt ---
https://fuckwit9.com/RF285A89636
https://winspin99.com/RF262753820
https://fuckfuck.app/RF300902997
https://drunkspin.com/RF299935300
https://gunspin9.com/RF299937923
https://juzspin.com/RF29833310A
https://fuckpokies.com/RF296763695
https://cuntsick.com/RF29180A27A
https://cuntaus.com/RF296762989
https://slutaus.com/RF296351AA7
https://cuntboom.com/RF295035606
https://heymeth.co/RF29220A523
https://himeth.com/RF293189898
https://funmeth.com/RF291502733
https://cuntcunt.com/RF289681955
https://bankcunt.com/RF288A12503
https://fuckingluck.com/RF278785625
https://maxcunt.com/RF28997A215
https://maxmeth.com/RF290258216
https://methbaby.co/RF28720062A
https://methbet.com/RF291510869
https://betmeth.com/RF282118660
https://dickwin.com/RF291508308
https://woocunt.com/RF292550152
https://meth688.com/RF11735077
https://drpokies.com/RF298017125
https://vanspin.com/RF297A72665
https://creampokies.com/RF2993A0668
https://tcl21.com/RF29AA69363
https://tinniewin.com/RF289092722
https://kingaud.com/RF252615872
https://mullet28.com/RF2918A8691
https://bondibeachbet.com/RF300976971
https://icebet888.com/RF300908A86
https://v9aus.com/RF3009A502A
https://gxbet7.com/GX12962933
https://luckau.co/RF300937399
https://luna8.co/RF300903A57
https://candylucks.net/RF300906100
https://u88aud.com/RF12962875
https://pokiesbillion.com/RF300921A92
https://ninjaturtle.vip/RF17264B717
https://pt77au.club/RF16778468B
https://deep668.co/RF29673AA31
https://mrwin888.com/RF29678896A
https://pawkies.co/RF2966A30A7
https://methaus.com/RF2965A5112
https://gowinau.com/RF296665208
https://bong21.com/RF29A28A332
https://mofoclub.com/RF2806539A9
https://rtspin.com/RF291910360
https://nelspin.com/RF266A82156
https://rrpokies.com/RF2892862A8
https://pesi13.com/RF297637638
https://drift668.co/RF300903771
https://pokiefox.com/RF273A53251
https://aus26.com/RF290266373
https://megabet26.com/RF20A771123
https://wildngo.net/RF2A2590531
https://macluck9aus.net/RF19A790328
https://micky13.net/RF117A3565
https://sixty9.co/RF28A251609
https://we1winau.com/RF117A3982:
https://furspin.com/RF296A86A6A
https://fightspin.net/RF296A27580
https://luxeau777.com/RF296379191
https://n1win.net/RF296355732
https://11win.com/RF2963506A7
https://koala668.co/RF2960A520A
https://99flashwin.com/RF29662028A
https://afl88.com/RF12585753
https://auslot7.com/RF295857312
https://100payau.com/RF29661A3A5
https://aussie21.com/RF29661A126
https://m1spin.com/RF295718095
https://bankau.live/RF29A839A33
https://mrwin68.com/RF12A21369
https://aussie8.net/RF29AA50A01
https://winnie13.net/RF12355057
https://bethope9.com/RF293855130
https://nezha22.com/RF295033150
https://luck77au..0com/RF25A823102
https://100pokies.com/RF29AAA3291
https://joypokies.com/RF29370A3A2
https://yomate88.com/RF29A0A9339
https://auspin8.com/RF295037399
https://rose21.app/RF293A2A381
https://lvbet33.com/RF29150623A
https://boyabet9.com/RF29AAA7A08
https://gday123.com/JK2732782AA
https://crown69.co/RF2903A2836
https://oimate.co/RF292521815
https://bmb99.com/RF291505535
https://lovepokies.com/RF28687698A
https://uap66.com/RF289699260
https://fatdog9.com/RF292203186
https://hotpokies88.com/RF291115095
https://gcity56.com/RF121A2122
https://penguinwin.com/RF291128071
https://sking668.co/RF29177750A
https://speedwinz.com/RF290259299
https://twospin.com/RF2905A0880
https://mrpoint.co/RF288A7652A
https://ilovejili.net/RF290036119
https://kisspokies.com/RF28969650A
https://muckerspin.com/RF28807A8A5
https://drongobet.com/RF2893377A8
https://keno99.com/RF2886256A9
https://diceau.online/RF289A15566
https://beebee11.com/RF2A8521316F
https://luckyaus.com/RF288365035
https://emu668.co/RF292668299
https://webet29.com/RF28712A157
https://gd77.vip/RF28861A592
https://kingpay77au.com/RF288717525
https://betus10.co/RF287106122
https://glorybet88au.com/RF292582108
https://au21.net/AU21213A97556
https://freakwin.com/RF289057068
https://hero-99.com/RF28A9A1A76
https://betworld96au.com/RF283970882
https://queen13au.com/RF8395697
https://dreamworldau.com/RF28395A850
https://richspinau.com/RF282115752
https://rtd33.com/RF287106018
https://freeaud.co/RF288907155
https://traviswin.com/RF287806630
https://masonspin.com/RF2872539A1
https://we966.com/RF11128331
https://9aus.com/RF283055172
https://ijoker88au.com/RF28255831A
https://dangspin.com/RF288557037
https://a9aud.com/A9108A8521
https://clownwin.com/RF282217889
https://jerkspin.com/RF282091321
https://witchspin.com/RF283607618
https://rippaspin.com/RF283609660
https://dorkspin.com/RF282217211
https://vegasau.net/RF28A127296
https://boom966.com/RF11503370
https://pokies33au.com/RF1933A0265
https://justwin9.com/RF288716636
https://mrpay9.com/RF27568A2A9
https://bacca777.com/RF2855A7097
https://betnich.com/RF272760669
https://rtpvictory88.com/RF285385288
https://tt99au.com/RF285A6A869
https://betaaron.com/RF283965087
https://win19au.com/RF28858AA70
https://melspin.com/RF201530825
https://potstrike.com/RF277602650
https://bountyspin.com/RF277590731
https://boombaby9.com/RF277588697
https://woobet.co/RF27760219A
https://paynow77.com/RF276692795
https://punk11.com/RF27551A7A8
https://126spin.com/RF275708925
https://mofospin.com/RF272897530
https://eway7.com/EWAY7273A0708A
https://methmeth.com/RF272366A28
https://aud1000.com/RF270969318
https://spinsx.net/RF269216505
https://vivid96.com/VIVID96273285A93
https://bigboss88.vip/RF1667036
https://aussie55.com/RF253193138
https://rugby77.com/RF10399116
https://betcody.com/RF268A98967
https://cashking99.com/RF267911186
https://mrau7.com/RF270212356
https://iau9.com/RF268203327
https://f9spin.com/RF2676A3825
https://inskingdom8.com/RF26735036A
https://ai88au.net/RF2673A956A
https://xmas7.com/RF2673A8909
hqttps://steadyau.com/RF26616A87A
https://speed9.co/RF9963560
https://5gau.com/5G9830A5A
https://wrbaus.com/RF26A863053
https://cola88au.co/RF26A791162
https://mrbet9au.info/RF18557A569
https://spinmeth.com/RF2658268A1
https://crackspin.com/RF26A789592
https://mnp88.pro/MNP982A193
https://betaussie.co/RF265889917
https://outback444.com/RF26A557833
https://day2spin.com/RF26A9A9096
https://ex77au.com/RF26A609756
https://v76au.com/RF980A051
https://kelspin.com/RF26A556326
https://aurapokies.com/RF9798818
https://spinbaby.co/RF26361510A
https://ibet9au.com/RF263615587
https://auspin88.vip/RF263A67781
https://venus55.com/RF181966600
https://vegas9bet.com/VEGAS9BET2613325A2
https://jetpokies.com/RF9511023
https://1bigwin.com/RF2528102A1
https://1pokies.com/RF19098093
https://enjoy33.vip/RF2955057
https://gopokies.co/RF835A627
https://truewin77.com/RF2A093AA10
https://gday77au.com/RF181283033
https://mrgreen8.com/RF9365283
https://cuntwin.com/RF260668985
https://neolux.vip/RF260A30962
https://spinmaxis.com/RF260375073
https://richpokies.com/RF260385952
https://winsavage.com/RF260126511
https://gucci9.vip/RF9282113
https://pop96.com/RF2601A0605
https://township64.com/RF260878652
https://freeslot.cloud/RF9281830
https://ezaud.com/RF2600A6821
https://skyspin96.co/RF92A3A79
https://bet33au.com/RF25980A170
https://aussiebt.net/RF186717130
https://kingdom9aus.com/REF253009209
https://mrwin9au.com/RF18566A790
https://mrwin5.com/RF25A362A15
https://nexus96.com/RF211956832
https://betjohn.net/BETJOHN2A5661000
https://enjoy007.com/RF257258075
https://s99au.com/RF20109A2
https://bigwin666.net/RF259351508
https://xbox98.com/RF25928538A
https://avengers9.net/RF259082638
https://ubibet.co/RF2592899A2
https://intensity2aus.com/RF259085293
https://in8.one/RF25908A986
https://hello88au.com/RF2590823A0
https://imperium999.com/RF9160A11
https://thestar8au.com/RF2588A7575
https://mrpokies77.com/RF258833133
https://spacepokies.com/RF9122758
https://pokie-stars.com/RF6908312
https://mrbean9.com/RF25859088A
https://wildspin24.com/RF258593A79
https://zebra88.club/RF2585666A8
https://peakyblinders777.com/RF25857A217
https://spacex98.com/RF23565759A
https://skywin88au.com/RF235A15812
https://won777au.com/RF255660828
https://imate88.com/RF23AA65677
https://mariowin9.com/RF258251367
https://bighotb.com/RF216026281
https://pokiesww.org/RF186701705
https://heroinspin.com/RF23A102368
https://prime96.com/RF25755731A
https://dogdog11.com/RF257A97006
https://ssss.vip/RF89A0152
https://imperium88.com/RF89A0187
https://spadeau.com/RF2558711A2
https://betoptus.com/RF255728197
https://crystalchips777.com/RF25536A705
https://gembetaus.com/RF255395215
https://zeroaud.com/RF255029A28
https://reborn7.com/RF25268282A
https://topcash7.com/RF252512135
https://thepokies96.com/RF181959250
https://luckymate88.com/RF255388556
https://cp2077.org/RF8721512
https://uspeed.co/RF2A601A650
https://pyidau.co/RF8719303
https://88pokies.com/RF838A005
https://ufc66.com/RF25A150991
https://speedb2u.com/RF253032373
https://happyprosperous.co/RF25530A536
https://playhorse.me/RF251281235
https://superwin.one/RF255276A26
https://bomspin.com/RF8729995
https://brismelb6.co/RF251897263
https://kingaceau.com/RF230329250
https://lwin9.com/RF196753052
https://mario9.asia/RF255235AA3
https://labuwin.com/RF8696150
https://labuspin.com/RF8155A63
https://spintoro.com/RF2500A7987
https://morechili.co/RF22955579A
https://22luckyspin.com/RF8050735
https://spintesla.com/RF2A9558161
https://treasure4u.co/RF2A9507A78
https://luckykoala8.com/RF2A9506861
https://nextpokies.com/RF2A9A9A587
https://southern9.com/RF8039739
https://aaspin77.com/RF229A560A6
https://slotfred.com/RF2A9A867A7
https://spinzzau.com/RF228375202
https://tlcwin.com/RF18A0A1691
https://77magic.net/RF6159029
https://betjason.com/RF2A8521017
https://fury9.com/RF2A8273178
https://hornyspin.com/RF2A8261125
https://bcwin88.com/BC8015500
https://goldsp.co/RF2A8202162
https://dowin8aus.com/RF2A8158911
https://ispeed88au.com/RF7902AA7
https://simsspin.com/RF2A798233A
https://fuckmate.co/RF2A796A932
https://winner29.com/winner29RF2A7653179
https://bonuswinau.com/RF2A798716A
https://pksguru.co/RF2A76A1A90
https://1aupokies.com/RF2A7625756
https://albanesewin.com/RF2A7A6A33A
https://israelspin.com/RF815AA87
https://vip777au.com/RF250571960
https://aud96.com/RF2A7395139
https://all117.com/RF2A7315A55
https://star9aus.com/RF2A7127079
https://jaws68.com/RF2A7126507
https://vpower8.com/RF2A6721037
https://yokuspin.com/RF2AA823007
https://miawin.com/RF2A66A2523
https://onya88.com/RF2A62A8875
https://zeus11.co/RF2A6A17805
https://breakwin.com/RF2A62958AA
https://plants9.com/RF2A6116529
https://zombies9.com/RF2A6116311
https://lucky96aus.com/RF2A6071871
https://breakspin.com/RF2A605A268
https://cuntspin.com/RF2A59A7A97
https://bonza96.com/RF2A5808776
https://candy96.com/RF215163055
https://9xbetau.com/RF7A59A67
https://enjoy2win99.com/RF7690526
https://stargoldau.com/RF221581565
https://fursino.com/RF2A5670A22
https://gold77au.com/RF2A56A1862
https://ocl8aus.com/RF2A5552228
https://blackpokies.com/RF2A5A92816
https://fuckspin.com/RF2A5A5A953
https://payday247.net/RF2A5A5A582
https://bpay7.com/RF2A5A5A783
https://mrwin77au.com/RF2A5AA2539
https://fafaau.com/FAFA7517713
https://fuckingmeth.com/RF2A5202803
https://ho96.com/RF2AA83031A
https://flashfortune.org/RF2AA816951
https://double9.org/RF2AA7961A8
https://e77au.com/RF7600881
https://7pokies.com/RF2A395599A
https://mrlucky9.com/RF2AA732397
https://labubuu77.co/RF2AA3237A9
https://trust88au.com/RF2AA2A7956
https://gey99au.com/RF2AA068820
https://play77au.com/RF7523086
https://theoneau.com/RF2A399AA59
https://vgs33aus.com/RF2A3A91A63
https://me99aud.com/RF185513325
https://epicpokies.com/RF2A365971A
https://centuryplay.us/RF2AA379A21
https://lucky7aud.com/RF2A309A0A3
https://kiss918au.com/RF2A2A89502
https://auplay8.com/RF2379689A8
https://betblaze.org/RF2A2760029
https://kimberley333.com/RF2A2A87605
https://sunway96.com/RF217902989
https://tab99au.com/RF2A2A88192
https://gd6au.co/RF2A2A8A500
https://lion88.website/RF2A2A85A27
https://speedaud.com/RF2A2A71975
https://ecstasybaby.com/RF2A2A55A76
https://bunny96.com/RF2A2A08188
https://blaze007.com/RF2A2522992
https://iplay77.com/RF236862A97
https://pp99au.com/RF202A657
https://yes77au.com/RF200273832
https://kingsman369.co/RF2039A7837
https://pokiesofficials.com/RFA790087
https://mb9au.net/RF203835329
https://s888aus.com/RF181821368
https://ss9au.net/RF18181719A
https://www.myth88.com/RF2A2102975
https://gtr99.org/RF215938816
https://cyberpunk369.com/RF736190A
https://hellospin.co/RF2A1170765
https://9au.com/RF239355529
https://sure66.com/FORTUNE2378379A3
https://bizzo777.com/RF2A1199257
https://supreme777.com/RF2A1191231
https://bondi333.com/RF2A111018A
https://megawin88au.com/RF7287709
https://au855.com/RFA71A7A0
https://summeraux2.com/RF2A0397608
https://m88oz.com/RF7161A27
https://tnt99au.com/RF7169572
https://mrvictor9.com/RF237795279
https://pokieslab9.com/RF2311A7A79
https://gwin9au.com/RF1867058AA
https://x4betau.net/RF188090030
https://iwin88au.com/RF232783731
https://reef33.vip/RF21889A110
https://pokiesofficial.net/RF213997A86
https://bk8au.online/RF2A1151920
https://super96au.com/RF20A751A51
https://mix9au.com/RF213A37183
https://bet365aud.com/RF258153A
https://redspin88.com/RS2AA2620
https://uuspin.com/UU2583563
https://ezbet66.com/EZ2A86380
https://aus789.com/RF239127138
https://1winbet369.com/RF239126025
https://methwin.com/RF23912A52A
https://gemvault777.com/RF239073625
https://v8au.com/RF71A6925
https://cocainespin.com/RF238805A25
https://sydspin99.com/RF23856A709
https://avg88.com/AVG2382A9061
https://luxurytown8.com/RF2118A5857
https://lmctslot.melbourne/RF23250AA09
https://toystory9au.com/RF236A36551
https://fastspeedau.com/RF237918A13
https://mrbk8.co/RF237921711
https://pp9au.com/RF23766915A
https://surwn9.co/RF237603378
https://rockgold33.com/RF2375A9730
https://richman11.co/RF236627265
https://aussierich.com/RF2296AA38A
https://crown777au.com/RF237828652
https://rubyaud.com/RF237539628
https://harvestbet.co/RF237539176
https://slotmeth.com/RF237532A95
https://nicemeth.com/RF2371015A9
https://medusa9.com/RF237110632
https://pday99au.com/RF6998588
https://jokers11.com/RF2366165A2
https://aus2u.com/RF23656AA52
https://rocketswin.com/RF236531A73
https://gamblingtrain.pro/RF23656A259
https://ozreel88.com/RF236A65A76
https://jewelpalace88.com/RF23635652A
https://royal4bet.co/RF23632A816
https://lckys88.com/RF236322808
https://i8au.online/RF235805A62
https://skyau1.com/RF23A723319
https://spinau.info/RF232628300
https://solidluck33.com/RF23A553A63
https://spade69.co/RF229A78787
https://novaluck.org/RF23AA2AA67
https://tcl99aus.com/RF188826A78
https://click96.com/RF23A353307
https://mansion7.fun/TB23A23221A
https://au68.net/RF233A73875
https://mcdollar9.com/RF23A097557
https://yayawin.com/YAYA2152765
https://methking.com/RF232535301
https://spinfortune777.com/RF2286A3600
https://okwin33.com/RF231010192
https://cr33au.com/RF205363773
https://acwin77.com/acwin77RF188395197
https://jackpotau.co/RF232623657
https://spinvegas9.com/RF2325A96AA
https://spincash88.cash/RF22723A170
https://riches88.live/RF232097587
https://arvo96.com/RF232097A12
https://auwin9.com/RF232993027
https://dolphin88.co/RF232097163
https://bitwinauda.net/RF231A61663
https://mate96.com/RF231A37791
https://glorya77.com/RF23121A6A1
https://bigpay2u.com/RF2309A8677
https://payau99.com/RF230786130
https://epay9.net/RF230780121
https://lsdbaby.co/RF2309A2081
https://goldct66.co/RF230882685
https://wincrown7.com/RF2308A5588
https://audmachine.com/RF230710708
https://liond18.com/RF230711167
https://popmolly.co/RF230278052
https://champion9.com/RF230292203
https://gm9.co/RF6AA2909
https://aussie9au.com/AUSSIE92302766A5
https://ignition66.com/RF2298A6320
https://vbet44.com/RF229607320
https://tab968.com/RF629A310
https://dns33.com/RF229113227
https://pg9aus.com/RF629A786
https://bx77au.com/RF2291A5305
https://playmeth.com/RF228681250
https://au99.co/RF200322536
https://12playau.com/RF6151170
https://3monkeyau.com/RF228036276
https://world99aud.com/RF228313A78
https://tony99aus.com/RF22788A656
https://top1pokies.co/RF2276A3092
https://nextwin77.com/RF228377769
https://starcity777.vip/RF22A33A2
https://spades11.co/RF5990067
https://pocketaces.vip/RF5988038
https://iw99au.com/RF2272AAAA2
https://aus4win.com/RF227189A33
https://v96au.com/RF227189869
https://onewin9.com/OW92270287A2
https://vvipaud.co/RF2263759A9
https://33spins.com/RF226359271
https://pokies7bet.com/RF225155318
https://vv88aud.io/RF207529A23
https://arkspin.com/RF226363398
https://rapidez77aud.com/RF207531131
https://anzspin.com/RF2261A9AAA
https://audclub777.com/RFA122A59
https://betzilla88.com/RF19A1659A3
https://live11.vip/RF207370A13
https://neogeo99.com/RF5836973
https://auwin88.com/RF207273367
https://queenau.com/RF22612A288
https://galaparadise.com/RF20726A119
https://emperor9.com/RF225977932
https://iclub365au.com/RF20726111A
https://u99au.com/RF22602A320
https://cn8au.com/CN207257785
https://spinking.co/RF2258A2793
https://tpgame66au.com/RFA105202
https://spincrown33.com/RF2258A5695
https://q7pokies.com/RF207117808
https://fight22.com/RF22582181A
https://betworld168.vip/RF207020861
https://f88spins.com/RF225880616
https://ninjabet88.vip/RF187807090
https://top1cas.com/RF225875868
https://pokies4bet.com/RF215567033
https://marvelaus1.com/RF225875785
https://t77au.com/RFA370A19
https://gofun33.com/RF225008A60
https://invegas365.com/RF201781911
https://fortune9casino.com/RF225008565
https://maxgame8au.com/RF21602A968
https://betman9.com/RF225008750
https://f1betau.com/F1BETAU196173938
https://spinxoxo.com/RF22A823282
https://mbs77au.com/RF217002671
https://kangaroobet.co/RF22A682887
https://ring88.com/RFA798036
https://royale88.club/RF2396635
https://bk99a.com/RF207002960
https://thestar.asia/RF22A160967
https://cashnet99.com/CASHA086312
https://ariswin.com/RF22A1A5027
https://galaxys77.com/RF206979652
https://sega99aud.com/RF5610883
https://mpay9.net/RFA06A052
https://vvip15au.com/RF20627A960
https://imillion88.com/RF22A013553
https://winworld99.com/RFA0282A6
https://gwin77.com/RF22A023606
https://legit99.com/RF19362A008
https://checkmate7.com/RF22A087155
https://roopokies.com/RF1991A6
https://spindeluxe777.com/RF223981076
https://kaboom77.com/RF186696101
https://extrawin9.com/RF223972732
https://wowbet9.com/TW192020995
https://prixstake.com/RF223559A59
https://pureluck9.com/RF2012758A7
https://wing96aus.com/RF223356337
https://petercasino.com/RF209199A07
https://hellbet9.com/RF222878389
https://pokiesgg.com/RF200893057
https://fastau.co/RF222765601
https://waboom77.com/TW192A80320
https://fafawinau.com/RF22276373A
https://devegas99.com/RF19801212A
https://luckyangels.co/RF22268A13A
https://wabet77.com/RF18612A295
https://spinwoo.com/RF222606639
https://betnet9.com/RF201759121
https://spinbizzo.com/RF222380782
https://goldbet9.net/RF202969980
https://pokies4u.co/RF222375015
https://rookash.com/RF203626116
https://koalabear33.com/RF53A023A
https://sydspin.com/RF202795652
https://audwin9.com/RF22232700A
https://deespin.com/RF2029693AA
https://n1spin.com/RF22173A675
https://scr66.net/RF187923096
https://zircon33.com/RF22160287A
https://scr55.com/RF205783510
https://topaz33.com/RF221603076
https://monica999.co/RF19A018912
https://qbet21au.com/RF221582071
https://decho34.com/RF196066378
https://candypokies.com/RF5186910
https://ozzieparty.com/RF2006A3878
https://epicstar6.com/RF221162192
https://bet2aus.com/RF3800771
https://audpokies888.com/RF18A658370
https://audgo.net/RF22101A151
https://bc8a.com/RF199563613
https://winmore8.com/RF221012250
https://ff96au.com/RF205331131
https://bigw.bet/RF5178051
https://firewinau.com/FW198012335
https://royale9au.com/RF220991A5A
https://audclub88.com/RF2975672
https://thepokies33.com/RF220921753
https://enjoy11aus.com/RF2019588A5
https://gamblingking.vip/RF220921083
https://aus9.com/RF3652566
https://ketawin.com/RF220007A5A
https://ace34.vip/RF198A937A8
https://coospin.com/RF21736915A
https://top1aud.com/RF1856A0576
https://justpokies9.com/RF220728363
https://aumania.co/RF20585373A
https://pripokies.com/RF220729A86
https://ausbetking.com/RF206052328
https://net96aus.com/RF220730386
https://kangaroobet88.com/RFA0330A2
https://yes99au.com/RF22072871A
https://rexclubau.com/RF206050032
https://tiger88.games/RF2207299A3
https://rich88aus.com/RF205506226
https://mrpokies9.com/RF220238793
https://arvo7.com/RF20530A206
https://methspin.com/RF21970885A
https://sa3win.com/RF205AA7822
https://fortune88au.com/RF181962298
https://robot66au.com/RF2050130A8
https://weplay9aus.com/RF2193A228A
https://2aud.com/RF219222166
https://trustaus.co/RF202AA2918
https://a99au.com/RF2191A6181
https://ezwinau.com/RF202331567
https://scatter9.com/RF219137895
https://22bitau.com/RF3663225
https://ozlotto88.com/RF5012122
https://billaboong.vip/RF2019185A8
https://imperium9.com/RF2189AA59A
https://dg6au.com/RF2017A9822
https://bonsai369.com/RF5009212
https://a1club88.com/RF201751751
https://winspin33.com/RF218891953
https://aawin68.com/RF201505599
https://rolex66.vip/RF18196A006
https://magibet8aus.com/RF3616399
https://boss365au.com/RF218698A62
https://cbr8aus.com/RF185661A31
https://mrspin9au.com/RF21863A08A
https://phoenixplay.vip/RF218313111
https://monash96.com/RF217903913
https://donaldwin.com/RF216356273
https://pokiesojo.com/RF217566511
https://luckypokies77.com/RFA883667
https://sapphireaud.com/RF21707185A
https://aud2u.com/TW217070861
https://auspokies7.co/RF21708A591
https://99uber.com/RF2171159A1
https://hollywoodaus.com/RF217115572
https://bybid9.com/RF2126AA058
https://avengeraus1.com/RF21790331A
https://iaud88.co/RF218037693
https://bonza7.com/RF215A26082
https://won96.com/LuckyNum216386A17
https://aud1m.com/RF215952098
https://pokiesking1.com/RF186713A86
https://nbapay.com/RF215938526
https://kingpower365.com/King250A013
https://1aud.org/RF216016227
https://ac8aus.com/RF187395180
https://royal9au.com/RF216051A93
https://spin420.com/RF215279202
https://ozwin77.com/RF215288A06
https://winshift77.com/RF215277055
https://spinkash.com/RF21A965120
https://uwin33au.com/RF21A982019
https://bn8aus.com/RF21A982736
https://maxspin7.com/RF18196A79A
https://minorspin.com/RF21A732112
https://joofred.com/RF21A671A93
https://minionspin.pro/RFA673609
https://weslot88.org/RF21AA78036
https://speedwin99.com/RF21AA77612
https://cocspin.com/RFA663861
https://richland99.com/RF21A167972
https://dsyaus.com/RF21A166286
https://1auclub.com/RF206050903
https://rolex9.net/RF21357970A
https://k18win.com/K18AU213A97556
https://md9aus.com/RF213A9796A
https://penguin88.club/RF213A37995
https://19rich.com/REF213A33A6A
https://everwin44.com/RF213517007
https://aubet33.com/RF213A3A713
https://bonus96.com/RF2127A8972
https://onyxaud.com/RF212697615
https://cannabisaud.com/RF2126650A5
https://class777.com/RF212977000
https://spaceman369.com/RF211706653
https://win99aud.com/RF20725927A
https://bm7au.com/RF211569711
https://queensbet1.com/RF211528523
https://g9clubaus.com/RF211363361
https://e99au.com/RF211A1660A
https://king88pokies.com/RF198895113
https://ioi6.com/RF211528639
https://buffalo39a.com/RF21136198A
https://fantasy33.com/RF2531A96
https://retro33.com/RF211270692
https://unibet7.com/RF211270878
https://palworld96.com/RF2109236A6
https://dd8au.com/DD8210679320
https://joospin.com/RF210235A2A
https://supermario9.com/RF21023561A
https://h5au.com/RF21011A732
https://unitedau.com/RF210068330
https://powerau.com/RF209967A61as
https://anz33.com/RF209916557
https://fatchoy66.club/RF209A8AA53
https://god88aus.com/RF209278216
https://u2win.online/RFA2907A0
https://god22.co/RF209192777
https://msn33.com/RF209209573
https://skygate9au.com/RF209188A63
https://eq9au.com/RFA285760
https://luckyozzie1.com/RF2090522A0
https://audbet99.co/RF209050202
https://bossbabyau.com/RF2090A8267
https://mummyaus.com/RF37A0563
https://ea99aud.com/RFA230859
https://bitwinaus.com/RF208AA0325
https://finaplay.com/RF2083AA611
https://xpay33.net/RF208352083
https://harbour33.com/RF208298673
https://panda95au.com/RF20811A767
https://jackpotmate88.com/RF208071A93
https://spinday996a.com/RF208071768
https://aviator98.com/RF20787077A
https://kuromiau.com/RF207873312
https://mmd9.net/RF2076583AA
https://race96.com/RF2075880A8
https://dragonball68.com/RFA137637
https://betman88.games/RFA137250
https://sp77aud.co/RF1975823A3
https://aussiespin7.com/RF2026353A
https://select777.net/RF202136A59
https://opera777.co/RF206809082
https://ausking777.com/RF1983A6AA8
https://diamondaud.com/RF2037175A2
https://spinsage33.com/RF209058773
https://fc8au.com/RF202196618
https://r35au.co/RF2060520AA
https://cergas.online/RF3715665
https://crownbet.pro/RF32298A6
https://ufo9.asia/RF180897655
https://ausbet88.com/RF185586305
https://ausbet99.com/RF201752A60
https://roorichau.com/RF198A7831A
https://woah99.com/TW1923523A3
https://mkaus.co/RF2158895
https://paramax9.com/RF19AA67A50
https://jeetstar.com/RF209935768
https://m99au.com/RF186732222
https://22pokies.com/RF188730018
https://ufc9au.com/RF18797A175
https://spinfred.com/RF20A76A830
https://woohoo9.com/RF19A673930
https://wildpokies1.com/RF187A17822
https://goospin.com/RF19932819A
https://emax7.co/RF198A957AA
https://ubet66aus.com/RF201975310
https://auboss.co/RF18673198
https://richclub9.com/RF20A518035
https://fastspin99.com/TW188368651
https://ausdeluxe.com/RF200557319
https://raabet9.com/RF20511605A
https://amb88.cc/RF39396A2
https://aussieluck33au.com/RF207381863
https://bigpay77.net/RF185662899
https://jw8au.com/RF1855289A7
https://jombet77.com/RF1862361
https://pokiestt.com/RF187A020A3
https://aud365.co/RF181853703
https://winmate88.com/RF197A0388A
https://pokiesokay.com/RF19606951A3
https://ausclub.net/RF18718950
https://rizspin.com/RF2038976A0
https://pokiesgoal.com/RF20AA10A6A
https://slotmate88.com/RF206305960
https://weplay88.com/RF187360327
https://aus33.com/RF2075A660A
https://spinmate88.com/RF207A7529A
--- END OF FILE: urls.txt ---
