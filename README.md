# Slap Red Scraper Alpha

The Slap Red Scraper Alpha is a Python-based tool designed to automate the collection of bonus and downline data from specified websites. It authenticates with target sites, fetches relevant information, and processes it into a structured format. Key capabilities include logging all operations, storing daily data, maintaining a historical archive of bonus information, generating daily comparison reports to track bonus changes, and providing a rich, dynamic console display for monitoring progress. The scraper is configurable via an INI file for credentials, target URLs, and operational settings.

## Features

*   **Automated Data Scraping**: Fetches bonus and (optionally) downline data from a list of specified URLs.
*   **User Authentication**: Securely logs into target sites using credentials provided in `config.ini`.
*   **Comprehensive Logging**:
    *   Detailed JSON-formatted logs for all significant events, including API requests/responses, errors, and job summaries.
    *   Logs are stored in the `logs/` directory (e.g., `logs/scrape.log`).
*   **Organized Data Output**: All generated data files are stored in the `data/` directory.
    *   **Daily Bonus CSVs**: Raw bonus data for the current day is saved in `data/[mm-dd] bonuses.csv`.
    *   **Historical Bonus Tracking**: Daily bonus CSVs are archived into `data/historical_bonuses.xlsx`, with each day's data on a separate sheet named `mm-dd`.
    *   **Daily Comparison Reports**: A CSV report (`data/comparison_report_[mm-dd].csv`) is generated, comparing the current day's bonuses against the previous day's data from the historical archive. This report categorizes bonuses as "New", "Used", "Persistent_Changed", or "Persistent_Unchanged", including details of what changed for persistent bonuses.
*   **Dynamic Console Display**:
    *   A rich, multi-line progress display updates in real-time in the console.
    *   Includes a graphical progress bar, percentage completion, per-site processing time, and total script run count.
    *   Shows detected bonus type flags (`[C]ommissions, [D]ownline First Deposit, [S]hare, [O]ther`) for each site.
    *   Provides detailed per-site statistics comparing current run vs. previous run for new and total items (Bonuses, Downlines, Errors).
*   **Run Metrics Caching**:
    *   Utilizes `data/run_metrics_cache.json` to store statistics from previous runs (total run count, per-site new/total items). This enhances the contextual information provided in the console display.
*   **Configurable Operation**:
    *   Control script behavior via `config.ini`, including credentials, target URL file, downline data fetching (enable/disable), and logging verbosity.
*   **Robust Error Handling**: Includes error detection for network issues, API errors, and data processing problems, with relevant information logged.

## Directory Structure

The scraper will automatically create the following directories in the project root if they don't already exist:

*   **`/data/`**: This directory is used to store all data files generated by the scraper.
    *   `[mm-dd] bonuses.csv`: Contains raw bonus data scraped on a specific date.
    *   `historical_bonuses.xlsx`: An Excel workbook where each sheet (named `mm-dd`) is an archive of a day's bonus data.
    *   `comparison_report_[mm-dd].csv`: A daily report comparing the day's bonuses to the previous day's, detailing new, used, and changed bonuses.
    *   `run_metrics_cache.json`: An internal file used by the script to store metrics from previous runs, enabling richer contextual information in the console display.

*   **`/logs/`**: This directory contains the log files generated by the scraper.
    *   `scrape.log` (or as configured in `config.ini`): The primary log file containing detailed JSON-formatted logs of the scraper's operations.

## Prerequisites

*   **Python**: Python 3.8 or higher is recommended. The script utilizes features common in modern Python versions.

## Setup & Configuration

This project is structured as a Python package (the `src` directory). For correct operation, especially when running the script as a module (`python -m src.main`), ensure that an empty file named `src/__init__.py` exists. This file being present allows Python to recognize `src` as a package. (This file should already be included in the repository).

1.  **Clone the Repository (if applicable):**
    If you have obtained the code as a Git repository, clone it to your local machine:
    ```bash
    git clone <repository_url>
    cd slap-red-scraper-alpha # Or your project directory name
    ```
    If you have the files directly, navigate to the project's root directory.

2.  **Install Dependencies:**
    The project uses several Python packages listed in `requirements.txt`. Install them using pip:
    ```bash
    pip install -r requirements.txt
    ```
    This will typically install packages like `requests`, `pandas`, and `openpyxl`.

3.  **Configure `config.ini`:**
    The main configuration for the scraper is done through the `config.ini` file located in the root directory of the project. Below is a description of each section and its parameters:

    *   **`[credentials]`**:
        *   `mobile`: Your mobile number used for logging into the target sites.
        *   `password`: Your password for the target sites.

    *   **`[settings]`**:
        *   `file`: The name of the text file containing the list of URLs to scrape (one URL per line). Example: `urls.txt`. This file should be in the root directory.
        *   `downline`: Set to `True` to fetch downline data, or `False` to fetch bonus data.

    *   **`[logging]`**:
        *   `log_file`: Path to the log file. It's recommended to use the default `logs/scrape.log` to store logs in the `logs` directory.
        *   `log_level`: The minimum logging level to record. Options include `DEBUG`, `INFO`, `WARNING`, `ERROR`, `CRITICAL`.
        *   `console`: Set to `True` to enable logging output to the console in addition to the log file. Set to `False` to disable console logging (note: the dynamic progress display will still print to console).
        *   `detail`: Controls the verbosity of structured log events. Options:
            *   `LESS`: Logs essential events like job start/complete and critical errors.
            *   `MORE`: Includes events like API requests/responses, successful fetches, and CSV writes.
            *   `MAX`: (Currently similar to `MORE`) Potentially for even more detailed future logging.

    Ensure `config.ini` is correctly filled out before running the scraper.

## Running the Scraper

After setting up your Python environment, installing dependencies, and configuring `config.ini`, it's important to run the scraper in a way that Python correctly recognizes its package structure (due to internal relative imports like `from .models import ...`).

**Recommended Method (from the project's root directory, e.g., `C:\py\`):**
```bash
python -m src.main
```
This command tells Python to run the `main.py` script as part of the `src` package, which ensures that relative imports within the `src` package work correctly.

**Alternative Method (may cause `ImportError`):**
You might also try running the script directly:
```bash
python src/main.py
```
However, this method can lead to an `ImportError: attempted relative import with no known parent package` if Python doesn't recognize `src` as a package from `main.py`'s perspective. This often happens if the `src` directory (or its parent) is not automatically added to Python's path in a way that resolves the package context for direct script execution.

**Upon execution (using the recommended method), the scraper will:**
1.  Read its configuration from `config.ini`.
2.  (The script relies on `src` being a package, typically ensured by an `src/__init__.py` file, for imports to function correctly.)
3.  Attempt to log in to each URL listed in your specified URL file.
4.  Fetch data (bonuses or downlines, based on `config.ini`).
5.  Display progress dynamically in the console.
6.  Save output data to the `/data` directory.
7.  Log operations to the `/logs` directory.

## Understanding the Output

The scraper produces output in two main forms: the dynamic console display during execution, and various files saved to the `/logs` and `/data` directories.

### Console Display

During execution, the scraper provides a 3-line dynamic display that updates for each URL being processed:

*   **Line 1: Overall Progress**
    *   `| █████----- | [ 50.00%] 10/20 |`
    *   Shows a text-based progress bar (default 40 characters wide, updating with block characters).
    *   Displays the percentage completion and the count of processed URLs versus the total.

*   **Line 2: Current Site & Run Info**
    *   `| 1.6s | [Run #5] | [C] N [D] Y [S] N [O] Y | [URL] https://examplesite.com |`
    *   **Site Processing Time**: Time taken to process the current URL (e.g., `1.6s`).
    *   **Run Count**: Total number of times the script has been executed (e.g., `[Run #5]`).
    *   **Bonus Type Flags**: Indicates types of bonuses found on the current site:
        *   `[C]`: Commissions (Y/N)
        *   `[D]`: Downline First Deposit bonuses (Y/N)
        *   `[S]`: Share bonuses (Y/N)
        *   `[O]`: Other types of bonuses (Y/N)
    *   **URL**: The `cleaned_url` currently being processed.

*   **Line 3: Per-Site Statistics**
    *   `| [B]|[R]:10/5(+5) [T]:100/90(+10) | [D]|[R]:-/- [T]:-/- | [E]|[R]:0/1(-1) [T]:5/6(-1) |`
    *   This line shows statistics for **B**onuses, **D**ownlines, and **E**rrors for the current site. Downline stats are only shown if `downline_enabled = True` in `config.ini`.
    *   **`[R]: cr/pr(±diff)`** (Run figures for this site):
        *   `cr`: New items found for this site in the current script run.
        *   `pr`: New items found for this site in the previous script run.
        *   `±diff`: Change between current and previous run's new items.
    *   **`[T]: crt/prt(±diff)`** (Total figures for this site):
        *   `crt`: Cumulative total items for this site up to and including the current run.
        *   `prt`: Cumulative total items for this site up to and including the previous run.
        *   `±diff`: Change in cumulative totals (should equal `cr`).
    *   A `-` (e.g., `[R]:-`) indicates that both current and previous values for that specific part (R or T) were zero, meaning no activity to report for that sub-metric.

### Log Files

*   **Location**: `logs/` directory (e.g., `logs/scrape.log`).
*   **Format**: JSON lines. Each line is a JSON object representing a log event.
*   **Content**: Detailed information about script operations, including API calls, errors, data fetching summaries, and job start/completion times. Useful for debugging and tracking.

### Data Files

All data files are stored in the `data/` directory.

*   **`[mm-dd] bonuses.csv`**:
    *   A CSV file created daily, containing all bonuses scraped on that particular date (`mm-dd`).
    *   Columns correspond to the fields of the `Bonus` data model (e.g., `url`, `merchant_name`, `id`, `name`, `amount`, `rollover`, etc.).

*   **`historical_bonuses.xlsx`**:
    *   An Excel workbook that serves as an archive of all daily bonus data.
    *   Each sheet in the workbook is named with the date (`mm-dd`) and contains the bonus data from that day (copied from the daily CSV).
    *   This allows for easy historical review and analysis.

*   **`comparison_report_[mm-dd].csv`**:
    *   A CSV file generated daily, providing a comparison of the current day's bonuses against the previous day's data (from `historical_bonuses.xlsx`).
    *   Key columns include:
        *   `status`: Indicates if a bonus is "New", "Used" (present yesterday, gone today), "Persistent_Changed", or "Persistent_Unchanged".
        *   `change_details`: For "Persistent_Changed" bonuses, this column lists the fields that changed and their old vs. new values (e.g., "amount: 10.0 -> 12.0; rollover: 1.0 -> 1.5").
        *   All original bonus data fields are also included.

*   **`run_metrics_cache.json`**:
    *   An internal file used by the script to maintain state between executions.
    *   Stores the `total_script_runs` count and, for each site, the new and total item counts from its previous run. This data is essential for the contextual statistics shown in the console display. It's not typically meant for direct user consumption but is vital for the script's enhanced display features.
